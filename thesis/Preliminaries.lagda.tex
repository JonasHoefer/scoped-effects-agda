% ...

\section{Agda}

Agda is a dependently typed functional programming language.
The current version\footnote{\url{https://github.com/agda/agda}} was originally
developed by Ulf Norell under the name Agda2 \cite{norell:thesis}.
Due to its type system Agda can be used as a programming language and as a proof
assistant.

This section contains a short introduction to Agda, dependent types and the idea
of ``Propositions as types'' under which Agda can be used for theorem proving.

\subsection{Basic Syntax}

Agda's syntax is similar to Haskells.
Data types are declared with syntax similar to Haskells GADTs.
Functions declarations and definitions are also similar to Haskell, except that
Agda uses a single colon for the typing relation.
In the following definition of \AgdaDatatype{ℕ}, \AgdaDatatype{Set} is the type
of all (small) types.

\begin{code}
data ℕ : Set where
  zero  : ℕ
  suc   : ℕ → ℕ
\end{code}
Ordinary function definitions are syntactically similar to Haskell.
Agda allows the definition of mixfix operators.
A mixfix operator is an nearly arbitrary list of symbols (builtin symbols like
colons are not allowed as part of operators).
Underscores in the operator name are placeholders for future parameters.
A mixfix operator can be applied partially by writing underscores for the
omitted parameters.

In the following definition of plus for natural numbers \AgdaFunction{+} is a
binary operator and therefore contains two underscores.

\begin{code}
_+_ : ℕ → ℕ → ℕ
zero   + m = m
suc n  + m = suc (n + m)
\end{code}
\begin{code}[hide]
{-# BUILTIN NATURAL ℕ #-}
\end{code}

\subsection{Dependent Types}
\label{preliminaries:dependent-types}
The following type theoretic definitions are taken from the homotopy type theory
book~\cite{hottbook}.
In type theory a type of types is called a universe.
Universes are usually denoted $\mathcal{U}$.
A function whose codomain is a universe is called a type family or a dependent
type.
$$
F : A \rightarrow \mathcal{U} \quad\text{where}\quad F(a) : \mathcal{U}
\quad\text{for}\quad a : A
$$
To avoid Russell's paradox, a hierarchy of universes $\mathcal{U}_1 :
\mathcal{U}_2 : \dots$ is introduced.
In Agda the universes are named \AgdaDatatype{Setₙ}, where \AgdaDatatype{Set₀}
can be abbreviated as \AgdaDatatype{Set}.
Usually the universes are cumulative i.e. if $\tau : \mathcal{U}_n$ then
$\tau : \mathcal{U}_k$ for $k>n$.
by default this is not the case in Agda.
Each type is member of a unique universe, but it's possible to lift a type to a
higher universe manually.
Since Agda 2.6.1 an experimental \texttt{--cumulativity} flag exists.

\paragraph{Dependent Function Types ($\Pi$-Types)} are a generalization of
function types.
The codomain of a $\Pi$ type is not fixed, but values with the argument the
function is applied to.
The codomain is defined using a type family of the domain, which specifies the
type of the result for each given argument.

$$
\prod_{a : A} B(a) \quad \text{with}\quad B : A \rightarrow\mathcal{U}
$$
An element of the above type is a function which maps every $a : A$ to a $b :
B(a)$.
In Agda the builtin function type $\rightarrow$ is a $\Pi$-type.
An argument can be named by replacing the type $\tau$ with $x : \tau$, allowing
us to use the value as part of later types.

\paragraph{Dependent Sum Types ($\Sigma$-Types)} are a generalization of product
types.
The type of the second component is not fixed, but varies with the value of the
first.
$$
\sum_{a : A} B(a) \quad \text{with}\quad B : A \rightarrow\mathcal{U}
$$
An element of the above type is a pair consisting of an $a : A$ and a $b : B(a)$.
In Agda \AgdaKeyword{record}s represent $n$-ary $\Sigma$-types.
Each field can be used in the type of the following fields.

\paragraph{Programming with Dependent Types}

A common example for dependent types are fixed length vectors.
The data type depends on a type \AgdaDatatype{A} and a value of type
\AgdaDatatype{ℕ}.

\begin{code}
data Vec (A : Set) : ℕ → Set where
  _∷_  : {n : ℕ} → A → Vec A n → Vec A (suc n)
  []   : Vec A 0
\end{code}
Arguments on the left-hand side of the colon are called parameters and are the
same for all constructors.
Arguments on the right-hand side of the colon are called indices and can differ
for each constructor.
Therefore, \AgdaDatatype{Vec}\AgdaSpace{}\AgdaArgument{A} is a family of types
indexed by \AgdaDatatype{ℕ}.

The \AgdaInductiveConstructor{[]} constructor allows us to create an empty
vector of any type, but forces the index to be zero.
The \AgdaInductiveConstructor{\_∷\_} constructor appends an element to the front
of a vector of the same element type and increases the index by $1$.
Only these two constructors can be used to construct vectors.
Therefore, the index is always equal to the amount of elements stored in the
vector.

By encoding more information about data in its type we can add extra constraints
to functions working with it.
The following definition of \AgdaFunction{head} avoids error handling or
partiality by excluding the empty vector as a valid argument.

\begin{code}
head : ∀ {A n} → Vec A (suc n) → A
head (x ∷ _) = x
\end{code}
When pattern matching on the argument of \AgdaFunction{head} there is no case
for \AgdaInductiveConstructor{[]}.
The argument has type \AgdaDatatype{Vec A (suc n)} and
\AgdaInductiveConstructor{[]} has type \AgdaDatatype{Vec A 0}.
Those to types cannot be unified, because \AgdaInductiveConstructor{suc} and
\AgdaInductiveConstructor{zero} are different constructors of \AgdaDatatype{ℕ}.
Therefore, the \AgdaInductiveConstructor{[]} case does not apply.
By constraining the type of the function we were able to avoid the case, which
usually requires error handling or introduces partiality.

We can extend this idea to type safe indexing.
A vector of length \AgdaArgument{n} is indexed by the first \AgdaArgument{n}
natural numbers.
The type \AgdaDatatype{Fin}\AgdaSpace{}\AgdaArgument{n} represents the subset of
natural numbers smaller than $n$.

\begin{code}
data Fin : ℕ → Set where
  zero  : {n : ℕ} → Fin (suc n)
  suc   : {n : ℕ} → Fin n → Fin (suc n)
\end{code}
Because $0$ is smaller than every positive natural number,
\AgdaInductiveConstructor{zero} can only be used to construct an element of
\AgdaDatatype{Fin}\AgdaSpace{}$($\AgdaInductiveConstructor{suc}
\AgdaArgument{n}$)$ i.e. for every type except
\AgdaDatatype{Fin}\AgdaSpace{}\AgdaArgument{0}.

If any number is smaller than $n$, then its successor is smaller than $n+1$.
Therefore, if any number is an element of
\AgdaDatatype{Fin}\AgdaSpace{}\AgdaArgument{n}
then its successor is an element of
\AgdaDatatype{Fin}\AgdaSpace{}$($\AgdaInductiveConstructor{suc}\AgdaSpace{}\AgdaArgument{n}$)$.

So we can construct a $k<n$ of type
\AgdaDatatype{Fin}\AgdaSpace{}\AgdaArgument{n} by starting with
\AgdaInductiveConstructor{zero} of type
\AgdaDatatype{Fin}\AgdaSpace{}$($\AgdaArgument{n - k}$)$ and applying
\AgdaInductiveConstructor{suc} $k$ times.
Using this definition of the bounded subsets of natural numbers we can define
a non partial version of \AgdaFunction{\_!\_} for vectors.

\begin{AgdaAlign}
\begin{code}
_!_ : ∀ {A n} → Vec A n → Fin n → A
(x  ∷ _ )  ! zero   = x
(_  ∷ xs)  ! suc i  = xs ! i
\end{code}
Notice that similar to \AgdaFunction{head} there is no case for
\AgdaInductiveConstructor{[]}.
\AgdaArgument{n} is used as index for
\AgdaDatatype{Vec}\AgdaSpace{}\AgdaArgument{A} and \AgdaDatatype{Fin}.
The constructors for \AgdaDatatype{Fin} only use \AgdaInductiveConstructor{suc},
therefore the type \AgdaDatatype{Fin}\AgdaSpace{}\AgdaInductiveConstructor{zero}
is not inhabited and the cases for \AgdaInductiveConstructor{[]} do not apply.

By case splitting on the vector first we could have obtained the term
\AgdaInductiveConstructor{[]}\AgdaSpace{}\AgdaFunction{!}\AgdaSpace{}\AgdaArgument{i}.
By case splitting on \AgdaArgument{i} we notice that no constructor for
\AgdaDatatype{Fin zero} exists.
Therefore, this case cannot occur, because the type of the argument is
uninhabited.
It's impossible to call the function, because we cannot construct an argument of
the correct type.
In this example we can either omit the case or explicitly state that the
argument is impossible to construct, by replacing it with $()$, allowing us to
omit the definition of the right-hand side of the equation.

\begin{code}
[]         ! () -- no right-hand side, because it's impossible to reach this case
\end{code}
\end{AgdaAlign}
The other two cases are straightforward.
For index \AgdaInductiveConstructor{zero} we return the head of the vector.
For index \AgdaInductiveConstructor{suc}\AgdaSpace{}\AgdaArgument{i} we call
\AgdaFunction{\_!\_} recursively with the smaller index and the tail of the
vector.
Notice that the types for the recursive call change.
The tail of the vector \AgdaArgument{xs} and the smaller index \AgdaArgument{i}
are indexed over the predecessor of \AgdaArgument{n}.

The above, quite simple examples can also be implemented in Haskell, for
example using type level natural numbers.
One of the advantages of Agda is the seamless and complete implementation of
dependent types.
For example, in contrast to a Haskell implementation we could simply reuse the
normal type of natural numbers to index other types.
Furthermore, we will later see more complex uses of dependent types, which
cannot be translated as easily to Haskell.


\subsection{Propositions as Types}

An more in depth explanation and an overview over the history of the idea can be
found in Wadlers paper of the same name \cite{DBLP:journals/cacm/Wadler15}.

The idea of propositions as types, also know as Curry-Howard correspondence,
relates type theory to logic.
As described by Wadler, it's a deep correspondents, relating \textit{propositions
to types}, \textit{proofs to programs} and \textit{simplification of proofs to
evaluation of programs}.
We will first consider the connection between the simple typed lambda calculus
(with product and sum types) and \textit{intuitionistic} propositional logic.
The intuitionistic version of propositional logic uses weaker axioms than
classical propositional logic.
For example, the law of the excluded middle and double negation elimination do
not hold in general.
Intuitively speaking, in intuitionistic logic every proof has to be witnessed.
When proving a disjunction one has to state which of the alternatives holds i.e.
one cannot just proof existence.
With this view point in mind it should be clear why intuitionists refute the law
of the excluded middle, if $A \vee \neg A$ were true in general one would
already have to know which alternative holds.

A proposition corresponds to a type, while the elements of the type correspond
to the proofs of the proposition.
A proposition is true iff the type representing it is inhabited.
Constructing a proof for a proposition means constructing a value of its type.
Unit and bottom type correspond to true and false, because one can always deduce
true (construct a value of the unit type $\top$) and there exist no proof of
falsity i.e. $\bot$ the type representing false is not inhabited.
Under the above interpretation the usual logical connectives like $\wedge$,
$\vee$ and $\Rightarrow$ correspond product, sum and function types.

For example, by the introduction rule for conjunctions, to construct a proof of
$A\wedge B$ one has to construct a proof of $A$ and a proof of $B$.
This corresponds to the construction of an element of the product type, because
to construct an element of $A\times B$ one has to provide an element of $A$ and
$B$.
The elimination rules for conjunction, i.e. one can deduce a proof of $A$ and a
proof of $B$ from the conjunction $A \wedge B$, corresponds to the projection
functions for products $\pi_1$ and $\pi_2$.
The argumentation for disjunction and sum/coproduct types is dual.

Functions from $A$ to $B$ map element of $A$ to elements of $B$ i.e. they
construct proofs of $B$ from proofs of $A$.
They correspond to implications.
Lambda abstracting of a value of type $A$ corresponds to assuming a proof of $A$
i.e. implication introduction, while application function corresponds to
implication elimination.

As explained in section \ref{preliminaries:dependent-types}, Agda's type system
has dependent types.
With dependent types the construction of a type (and therefore whether the type
is inhabited) can change based on a value.
Dependent types therefore correspond to predicates.
$\Sigma$ and $\Pi$ types correspond to existential and universal quantification.
To proof an existential one has to provide a value and a proof that the
proposition holds for this value.
These two correspond to the two elements of a the $\Sigma$ type.
The type of the second element depends on the first one i.e. it's a proof for a
predicate on the first value.
The prove a universal quantified proposition one has to proof that it holds for
every possible value.
A $\Pi$ type is a function whose return type depends on the given value i.e.
given any value it returns a proof for the predicate on its argument.

Extending the type system with dependent types corresponds to extending
intuitionistic propositional logic to intuitionistic predicate logic.
Using the above corresponds Agda can be used to state and proof theorems in
predicate logic.
An overview over the correspondences between certain types and connectives, as
well as the corresponding syntax in Agda can be found in table
\ref{preliminaries:propositions-as-types:table}.

\begin{table}
  \begin{center}
    \begin{tabular}{ccc}
      FOL & MLTT & Agda \\\hline
      $\forall x \in A: P(x)$ & $\Pi_{x:A}P(x)$ & \mintinline{agda}{(x : A) → P x} \\
      $\exists x \in A: P(x)$ & $\Sigma_{x:A} P(x)$ & \mintinline{agda}{Σ[ x ∈ A ] P x} \\
      $\neg A$ & $A \rightarrow 0$ & \mintinline{agda}{A → ⊥} \\
      $P \wedge Q$ & $P \times Q$ & \mintinline{agda}{A × B} \\
      $P \vee Q$ & $P + Q$ & \mintinline{agda}{A ⊎ B} \\
      $P \Rightarrow Q$ & $P \rightarrow Q$ & \mintinline{agda}{A → B} \\
      $\mathbf t$ & $\mathbf 1$ & \mintinline{agda}{⊤} \\
      $\mathbf f$ & $\mathbf 0$ & \mintinline{agda}{⊥}
    \end{tabular}
  \end{center}
  \label{preliminaries:propositions-as-types:table}
  \caption{correspondence between first order logic and types in mathematical
    and Agda notation}
\end{table}

\subsection{Notions of Equality and Equality Types}

In the last section we saw how we can encode propositions from propositional and
predicate logic as types.
One of the most important proposition is equality i.e. the proposition that
given two terms $a, b : A$ that $a$ and $b$ are equal.
When using Agda for theorem proving we have to express propositions like $a + b
= b + a$ and $2 = 1$, which could be true or false, to be able to prove or
disprove them.
In type theory and therefore in Agda we have to consider different notions of
equality.

When defining a program rule like
\AgdaFunction{truth}\AgdaSpace{}\AgdaSymbol{=}\AgdaSpace{}\AgdaNumber{42}
we are making an \textit{equality judgement}.
The symbol \AgdaFunction{truth} is \textit{definitional equal} to \AgdaNumber{42}.
A judgment is always true.
We define one term to be equal to another one i.e. we allow Agda to reduce the
left-hand side of the equality to the right-hand side.

The next notion of equality is \textit{computational equality}.
Two terms $t_1$ and $t_2$ are computational equal if they reduce to the same
term.
For example, given the above definition of $+$ the terms $0+(0+n)$ and $n$ are
computational equal, because using the first rule in the definition of plus
$0+(0+n)$ $\beta$-reduces to $n$.
On the other hand $n+0$ and $n$ are not computational equal, because for a free
variable $n$ none of the program rules for $+$ can be used to reduce the term
further.

To talk about the equality of two terms we have to use \textit{propositional
equality} i.e. we have to define a proposition representing the fact that two
terms of type $A$ are equal.
This proposition is usually encapsulated in an \textit{equality type} of $A$.

\begin{code}
infix 4 _≡_
data _≡_ {A : Set} (x : A) : A → Set where
  refl : x ≡ x
\end{code}
The type
\AgdaArgument{x}\AgdaSpace{}\AgdaDatatype{≡}\AgdaSpace{}\AgdaArgument{y}
represents the proposition that $x$ and $y$ are equal.
The only way to construct evidence for the proposition is using the
\AgdaInductiveConstructor{refl} constructor i.e. if $x$ and $y$ are actually the
same.

This notion of equality has the usually expected properties like transitivity,
symmetry and congruence.
The definition of \AgdaFunction{cong} shows the typical way of working with
equality proofs.

\begin{code}
cong : ∀ {A B x y} → (f : A → B) → x ≡ y → f x ≡ f y
cong f refl = refl
\end{code}
We expect that $≡$ is a congruence relation i.e. if $x$ and $y$ are equal then
$f x$ and $f y$ should also be equal.
We cannot produce a value of type
\AgdaArgument{f}\AgdaSpace{}\AgdaArgument{x}\AgdaSpace{}\AgdaDatatype{≡}\AgdaSpace{}\AgdaArgument{f}\AgdaSpace{}\AgdaArgument{y}
because the two are not equal.
By pattern matching in the argument of type 
\AgdaArgument{x}\AgdaSpace{}\AgdaDatatype{≡}\AgdaSpace{}\AgdaArgument{y}
i.e. by inspecting the evidence that $x$ and $y$ are equal we obtain more
information about the goal.
Because \AgdaInductiveConstructor{refl} can only be constructed if the two
values are the same the two variables are unified.
We therefore have to produce a proof that $f x$ is equal to itself, which is
given by reflexivity.

By pattern matching on variables used in the equality type we can obtain more
information about the goal.
Either because the constructors them-self restrict the use of the variables or
because the terms used in the equality type can be reduced further.
Consider the following example.

\begin{code}
+-identʳ : ∀ n → n + 0 ≡ n
+-identʳ 0        = refl
+-identʳ (suc m)  = cong suc (+-identʳ m)
\end{code}
By pattern matching on the variable $n$ we obtain two cases, one for each
constructor (this is analogous to a proof by exhaustion).
In both cases the term
\AgdaArgument{n}\AgdaSpace{}\AgdaFunction{}\AgdaSpace{}\AgdaNumber{0}
can now be reduced further.
In the \AgdaNumber{0} case we obtain
\AgdaNumber{0}\AgdaSpace{}\AgdaFunction{+}\AgdaSpace{}\AgdaNumber{0}
on the left-hand side, which can be reduced to \AgdaNumber{0}.
The return type simplifies to 
\AgdaNumber{0}\AgdaSpace{}\AgdaDatatype{≡}\AgdaSpace{}\AgdaNumber{0}
for which we can simply construct evidence using
\AgdaInductiveConstructor{refl}.

The second case is more complex.
The left hand side still contains the free variable $m$, but reduces to
\AgdaInductiveConstructor{suc}\AgdaSpace{}$($\AgdaArgument{m}\AgdaSpace{}\AgdaFunction{+}\AgdaSpace{}\AgdaNumber{0}$)$
using the second rule for \AgdaFunction{\_+\_}.
Using a recursive call we obtain evidence for
\AgdaArgument{m}\AgdaSpace{}\AgdaFunction{+}\AgdaSpace{}\AgdaNumber{0}\AgdaSpace{}\AgdaDatatype{≡}\AgdaSpace{}\AgdaArgument{m}.
The recursive call to obtain evidence for a smaller case corresponds to the use
of the induction hypothesis in an inductive proof.
By applying \AgdaInductiveConstructor{suc} on both sides of the equality we
obtain a proof for the correct proposition.

We obtained a non obvious equality by using just definitional and computational
equality together with the analogs of proofs by exhaustion and induction.
This proof can now be used to rewrite arbitrary terms containing terms
corresponding to or containing the left- or right-hand of the equality.

The above definition of propositional equality defines so called
\textit{intentional} propositional equality.
Intentional equality respects how objects are defined.
Extensional equality just observes how objects behave.
This distinction is important when comparing functions, as we will do in nearly
all later proofs.
Two functions which behave identical but are defined differently are
extensional but not intentional equal.
In Agda one can use extensional equality be arguing in another setoid (i.e.
using a different equivalence relation, which assumes that extensionality holds)
or by adding an axiom to modify the underlying theory.
For simplicity we will do the later and postulate the axiom of extensionality.

\begin{code}
postulate
  extensionality : ∀ {A : Set} {B : A → Set} 
    {f g : (x : A) → B x} → (∀ (x : A) → f x ≡ g x) → f ≡ g
\end{code}
The axiom simply states that the two functions \AgdaArgument{f} and
\AgdaArgument{g} are considered equal if they are equal point-wise.
It extends the intentional propositional equality to the extensional one, while
preserving Agda's consistency.


\subsection{Termination Checking}
\label{sec:termination-checking}

Non-terminating functions correspond to circular arguments.
Therefore, allowing the definition of non-terminating functions entails logical
inconsistency.
When defining functions Agda allows general recursion, but only terminating
functions are valid Agda programs.
Due to the undecidability of the halting problem Agda uses a heuristic
termination checker.
The termination checker proofs termination by observing structural recursion.
Consider the following definitions of \AgdaDatatype{List} and
\AgdaFunction{map}.

\begin{code}
data List (A : Set) : Set where
  _∷_  : A → List A → List A
  []   : List A

map : {A B : Set} → (A → B) → (List A → List B)
map f (x ∷ xs)  = f x ∷ map f xs
map f []        = []
\end{code}
The \AgdaInductiveConstructor{[]} case does not contain a recursive call.
In the \AgdaInductiveConstructor{\_∷\_} case the recursive call to
\AgdaFunction{map} occurs on a structural smaller argument i.e.
\AgdaArgument{xs} is a subterm of the argument
\AgdaArgument{x}\;\AgdaInductiveConstructor{∷}\;\AgdaArgument{xs}.
Because elements of \AgdaDatatype{List A} are finite the function
\AgdaFunction{map} terminates for every argument.


\subsubsection{Sized Types}
\label{preliminaries:sized-types}

In more complex recursive functions the structural recursion can be obscured.
For example, Agda does not inline functions containing pattern matches during
termination checking.
A common example are recursive calls in lambdas, which are passed to higher
order functions like \AgdaFunction{map} and \AgdaFunction{>>=}.

To still proof termination in such cases we can resort to type-based
termination checking as described by Abel~\cite{DBLP:journals/lmcs/Abel08}.
The fundamental idea is to represent the size of a recursive data structure in
its type.
Termination of recursive functions can be proven by observing a reduction in
size on the type level.
Abel noted as main advantages robustness with respect to small/local changes in
code (which do not impact the type) as well as scaling to higher-order functions
and polymorphism~\cite{DBLP:journals/lmcs/Abel08}.
The latter will be essential in future chapters, since representations of
computational effects involve complex constructs, which obscure recursion on the
value level.

Type-based termination is available in Agda in the form of \textit{sized types}.
The special, builtin type \AgdaDatatype{Size} can be used to annotate data types
with a value which represents an upper bound on there size. % :/
A set of builtin functions and types can be used to establish relations between
different sizes.
The elements of \AgdaDatatype{Size} are well ordered.
\AgdaFunction{↑\_} is the successor operation on \AgdaDatatype{Size} i.e. 
\AgdaArgument{i}$\;<\;$\AgdaFunction{↑}\AgdaSpace{}\AgdaArgument{i}.
\AgdaFunction{\_⊔ˢ\_} is the supremum w.r.t. the relation.
\AgdaInductiveConstructor{∞} is the top element in \AgdaArgument{Size}.
Given an \AgdaArgument{i} the type of all smaller sizes can be constructed using
\AgdaDatatype{Size<\_}\footnote{We will not use this construction. It's usually
  used when working with \AgdaKeyword{coinductive} data types.}.
If the size decreases across all recursive call of a function termination
follows from the well-ordering on \AgdaDatatype{Size}.

\begin{code}[hide]
open import Agda.Builtin.Size public
  renaming ( SizeU to SizeUniv )  --  sort SizeUniv
  using    ( Size                 --  Size   : SizeUniv
           ; Size<_               --  Size<_ : Size → SizeUniv
           ; ↑_                   --  ↑_     : Size → Size
           ; _⊔ˢ_                 --  _⊔ˢ_   : Size → Size → Size
           ; ∞ )                  --  ∞      : Size
\end{code}

A common idiom for data types is to mark all non-inductive constructors and
recursive occurrences with an arbitrary size \AgdaArgument{i} and all inductive
constructors with the next larger size
\AgdaFunction{↑}\AgdaSpace{}\AgdaArgument{i}.
The size therefore corresponds to the height of the tree described by the term
($+$ the initial height for the lowest non-inductive constructor).

A common example for sized types are rose trees.
The size index can be intuitively thought of as the height of the tree.

\begin{code}
data Rose (A : Set) : Size → Set where
  rose : ∀ {i} → A → List (Rose A i) → Rose A (↑ i)
\end{code}
When \AgdaFunction{fmap} for rose trees is defined in terms of
\AgdaFunction{map} the termination is obscured.
The argument of the functions passed to \AgdaFunction{map} is not recognized as
structurally smaller than the given rose tree.
Using size annotations we can fix this problem.

\begin{code}
map-rose : {A B : Set} {i : Size} → (A → B) → (Rose A i → Rose B i)
map-rose f (rose x xs) = rose (f x) (map (map-rose f) xs)
\end{code}
By pattern matching on the argument of type
\AgdaDatatype{Rose}\AgdaSpace{}\AgdaArgument{A} of size
\AgdaFunction{↑}\AgdaSpace{}\AgdaArgument{i} we obtain a \AgdaDatatype{List} of
trees of size \AgdaArgument{i}.
The recursive calls via \AgdaFunction{map} therefore occur on smaller trees.
Therefore the functions has to terminates.

In this case inlining the call of \AgdaFunction{map} would also solve the
termination problem.
When generalizing the definition of the tree from \AgdaDatatype{List} to an
arbitrary functor this wouldn't be possible.
In many cases inlining all helper functions is either not feasible or would lead
to large and unreadable programs.


\subsection{Strict Positivity}

In a type system with arbitrary recursive types, it is possible to to implement a
fixpoint combinator and therefore non terminating functions without explicit
recursion.
As explained in section \ref{sec:termination-checking} this entails logical
inconsistency.
Agda allows only strictly positive data types.
A data type $D$ is strictly positive if all constructors are of the form
$$
  A_1 \rightarrow A_2 \rightarrow \dots \rightarrow A_n \rightarrow D
$$
where $A_i$ is either not inductive (does not mention $D$) or are of the form
$$
A_1 \rightarrow B_2 \rightarrow \dots \rightarrow B_n \rightarrow D
$$
where $B_j$ is not inductive.
By restricting recursive occurrences of a data type in its definition to strict
positive positions strong normalization is preserved.

\subsubsection{Container}
\label{container}
% TODO: mention INLINE trick for map

Because of the strict positivity requirement it is not allowed to apply generic
type constructors to inductive occurrences of a data type in its definition.
The reason for this restriction is that a type constructor is not required to
use its argument only in strictly positive positions.
To still work generically with type constructors or more precise functors we
need a more restrictive representation, which only uses its argument in a
strictly positive position.
One representation of such functors are containers.

Containers are a generic representation of data type, which store values of an
arbitrary type.
They were introduced by Abbott, Altenkirch and Ghani
\cite{DBLP:conf/fossacs/AbbottAG03}.
A container is defined by a type of shapes $S$ and a type of positions for each
of its shapes $P : S \rightarrow \mathcal{U}$.
Usually containers are denoted $S \rhd P$.
A common example are lists.
The shape of a list is defined by its length, therefore the shape type is
$\mathbb{N}$.
A list of length $n$ has exactly $n$ places or positions containing data.
Therefore, the type of positions is $\Pi_{n : \mathbb{N}}\mathrm{Fin}\;n$ where
$\mathrm{Fin}\;n$ is the type of natural numbers smaller than $n$.
The extension of a container is a functor $\lBrack S \rhd P \rBrack $,
whose lifting of types is given by

$$
  \lBrack S \rhd P \rBrack\;X = \sum\limits_{s : S} P s \rightarrow X.
$$
A lifted type corresponds to the container storing elements of the given type
e.g. $\lBrack\, \mathbb{N} \rhd \mathrm{Fin}\;\rBrack \; A \cong
\mathrm{List}\;A$.
The second element of the dependent pair sometimes called position function.
It assigns each position a stored value.
The functors action on functions is given by

$$
  \lBrack S \rhd P \rBrack \;f\;\langle s , pf \rangle = \langle s , f \circ pf \rangle.
$$
We can translate these definition directly to Agda.
Instead of a \AgdaKeyword{data} declaration we can use \AgdaKeyword{record}
declarations.
Similar to other languages \AgdaKeyword{record}s are pure product types.
A \AgdaKeyword{record} in Agda is an $n$-ary dependent product type i.e. the
type of each field can contain all previous values.

\begin{code}
record Container : Set₁ where
  constructor _▷_
  field
    Shape : Set
    Pos : Shape → Set
open Container public
\end{code}
As expected, a container consists of a type of shapes and a dependent type of
positions.
Notice that \AgdaDatatype{Container} is an element of \AgdaDatatype{Set₁},
because it contains a type from \AgdaDatatype{Set} and therefore has to be
larger.
Next we define the lifting of types i.e. the container extension, as a function
between universes.

\begin{code}
open import Data.Product using (Σ-syntax; _,_) -- TODO: define and explain earlier
open import Function using (_∘_)
⟦_⟧ : Container → Set → Set
⟦ S ▷ P ⟧ A = Σ[ s ∈ S ] (P s → A)
\end{code}
Using this definition we can define \AgdaFunction{fmap} for containers.

\begin{code}
fmap : ∀ {A B C} → (A → B) → (⟦ C ⟧ A → ⟦ C ⟧ B)
fmap f (s , pf) = (s , f ∘ pf)
\end{code}

\section{Curry}
\label{preliminaries:curry}

Curry \cite{Hanus95curry} is a functional logic programming language.
It combines paradigms from functional programming languages like Haskell
with those from logical languages like Prolog.
Curry is based on Haskell i.e. its syntax and semantics not involving
nondeterminism closely resemble Haskell.
Curry integrates logical features, such as nondeterminism and free variables
with a few additional concepts.

Like in Haskell, functions are defined using equations, but curry assigns
different semantics to overlapping clauses.
When calling a function all right-hand sides of matching left-hand sides are
executed.
This introduces non determinism.
Nondeterminism is integrated as an ambient effect i.e. the effect is not
represented at type level.
The simplest example of a nondeterministic function is the choice operator
\texttt{?}.

\begin{minted}{haskell}
     (?) :: A -> A -> A
     x ? _ = x
     _ ? y = y
\end{minted}
Both equations always match, therefore both arguments are returned i.e.
\texttt{?} introduces a nondeterministic choice between its two arguments.
Using choice we can define a simple nondeterministic programs.

\begin{minted}{haskell}
     coin :: Int
     coin = 0 ? 1

     twoCoins :: Int
     twoCoins = coin + coin
\end{minted}
\texttt{coin} chooses non-deterministically between $0$ and $1$.
Executing \texttt{coin} therefore yields these two results.
When executing \texttt{twoCoins} the two calls of \texttt{coin} are independent.
Both choose between $0$ and $1$, therefore \texttt{twoCoins} yields the results
$0$, $1$, $1$ and $2$.

\subsection{Call-Time-Choice}
\label{call-time-choice}
Next we will take a look at the interactions between nondeterminism and function
calls.

\begin{minted}{haskell}
     double :: Int -> Int
     double x = x + x
     
     doubleCoin :: Int
     doubleCoin = double coin
\end{minted}
When calling \texttt{double} with a nondeterministic value two behaviors are
conceivable.
The first possibility is that the choice is moved into the function i.e. both
\texttt{x} chose independent of each other yielding the results $0$, $1$, $1$
and $2$.
The second possibility is choosing a value before calling the function and
choosing between the results for each possible argument.
In this case both \texttt{x} have the same value, therefore the possible results
are $0$ and $2$.
The later option is called Call-Time choice and is the one implemented by Curry.

Similar to Haskell, Curry programs are evaluated lazily.
The evaluation of an expression is delayed until its result is needed and each
expression is evaluated at most once.
The later is important when expressions are named and reused via \texttt{let}
bindings or lambda abstraction.
The named expression is evaluated the first time it is needed.
If the result is needed again the old value is reused.
This behavior is called sharing.
Usually function application is defined using the \texttt{let} primitive.
Applying a non variable expression to a function introduces a new intermediate
result, which bound using \texttt{let}.
$$
(\lambda x.\sigma) \tau = \texttt{let}\;y = \tau\;\texttt{in}\;\sigma[x\mapsto y]
$$
We therefore expect a variable bound by a \texttt{let} to behave similar to one
bound by a function.
This naturally extends Call-Time choice to \texttt{let}-bindings in lazily
evaluated languages.

\begin{minted}{haskell}
     sumCoin :: Int
     sumCoin = let x = coin in x + x
\end{minted}
As expected this function yields the results $0$ and $2$.


\section{Algebraic Computational Effects}

Giving a concrete definition for computational effects is hard.
Examples include I/O, exceptions, nondeterminism, state and delimited
continuations~\cite{DBLP:journals/corr/abs-1807-05923}.
Algebraic effects are computational effects, which can be described using an
algebraic theory.
They were first presented by Plotkin and
Power~\cite{DBLP:conf/fossacs/PlotkinP02}.

Handlers for algebraic effects were first presented by Plotkin and Pretnar
as a generalization of exception handlers~\cite{DBLP:conf/esop/PlotkinP09}.
Handlers can be used to describe non-algebraic operations, which include
operations with scopes such as \texttt{catch} and \texttt{once}.

Wu et al. describe a problem with this approach when multiple handlers
interact.
The ordering of handlers induces a semantic for the program, but simultaneously
the handlers also delimit scopes.
The correct ordering for scoping and semantics maybe not coincide.
Wu et al. fix this problem by introducing new syntactic constructs to delimit
scopes, removing the responsibility from the
handler~\cite{DBLP:conf/haskell/WuSH14,DBLP:conf/lics/PirogSWJ18}.


\subsection{Algebraic Theories}

The following definition is similar to the one given by
\textcite{DBLP:journals/corr/abs-1807-05923}.

An algebraic theory consists of a signature describing the syntax of the
operations together with a set of equations.
A signature is a set of operation symbols together with an arity set and an
additional parameter.
Operations of this form are usually denoted with a colon and $\leadsto$
between the three sets, suggesting that they describe special functions.
\[
  \Sigma = \{ \mathrm{op}_i : P_i \leadsto A_i  \}
\]
Given a signature $\Sigma$ we can build terms over a set of variables $\mathbb{X}$.
\[
  x \in \mathrm{Term}_\Sigma(\mathbb{X}) \quad\text{for}\quad x\in\mathbb{X} \qquad
  \mathrm{op}_i(p, \kappa) \in \mathrm{Term}_\Sigma(\mathbb{X}) \quad\text{for}\quad
  p\in P_i, \kappa : A_i \rightarrow \mathrm{Term}_\Sigma(\mathbb{X})
\]
Terms can be used to form equations of the form $x\;|\;l=r$.
Each equation consists of two terms $l$ and $r$ over a set of variables $x$.
A signature $\Sigma_T$ together with a set of equations $\mathcal{E}_T$ forms an
algebraic theory $T$.
\[
  T = (\Sigma_T, \mathcal{E}_T)
\]
An interpretation $I$ of a signature is given by a carrier set $|I|$ and an
interpretation for each operation $\lBrack\cdot\rBrack_I$.
An interpretation of an operation $\mathrm{op}_i$ is given by a functions to the
carrier set $|I|$, that takes an additional parameter from $P_i$ and $|A_i|$
parameters as a function from the arity set to the carrier set $|I|$.
\[
  \lBrack \mathrm{op}_i \rBrack_I : P_i \times |I|^{A_i} \rightarrow |I|
\]
Given a function $\iota : \mathbb{X} \rightarrow |I|$, assigning each variable
a value, we can give an interpretation for terms $\lBrack\cdot\rBrack_{(I,\iota)}$.
\begin{align*}
  \lBrack x \rBrack_{(I,\iota)} &= \iota(x) \\
  \lBrack \mathrm{op}_i(p, \kappa)\rBrack_{(I,\iota)} &= \lBrack op_i \rBrack_I (p, \lambda p. \lBrack\kappa(p)\rBrack_{(I, \iota)})
\end{align*}
An interpretation for $T$ is called $T$-model if it validates all equations in $\mathcal{E}_T$.

% \paragraph{Algebraic Theories for Computational Effects}
% 
% \begin{itemize}
%   \item Effects which can be described using an algebraic theory
%   \item evaluation order from top to bottom
%   \item interpretation of operations essential performs side effect
%   \item $|I|^{A_i}$ corresponds to continuation, i.e. rest of the computation
%     processing the result
%   \item if $A_i$ is infinite (e.g. for \texttt{get} it is $S$) it describes possible
%     continuations for each state (contrast: in math operations usually process
%     all parameters $\Leftrightarrow$ results of subcomputations)
% \end{itemize}

\subsubsection{Free Model}

For each algebraic theory $T$ we can generate a so called free model
$\mathrm{Free}_T(\mathbb{X})$.
The free model is the optimal model for the theory i.e. it validates just
equations from $\mathcal{E}_T$ and the ones directly following from them and no
more.

The set $\mathrm{Tree}_T(\mathbb{X})$ contains term trees, built from the
variables in $\mathbb{X}$ and the operations of the signature $\Sigma_T$.
\[
  \mathrm{pure}\;x \in \mathrm{Tree}_T(\mathbb{X}) \qquad \mathrm{op}_i(p,\kappa)
  \in \mathrm{Tree}_T(\mathbb{X}) \quad\text{for}\quad p\in P_i \quad \kappa :
  A_i \rightarrow \mathrm{Tree}_T(\mathbb{X})
\]
The free model for $T$ is given by

\[
  \mathrm{Free}_T(\mathbb{X}) = \mathrm{Tree}_T(\mathbb{X})\; /\;\sim_T
\]
where $\sim_T$ is the least equivalence relation, such that for all equations
$x\;|\;l=r\in\mathcal{E}_T$, $l\sim_T r$.
Furthermore $\sim_T$ is required to be a congruence relation for the operations
of the signature i.e.
\[
  \forall i\in\{1,\dots ,n\}. x_i\sim_T y_i \Rightarrow \mathrm{op}(x_1,\dots , x_n)
  \sim_T \mathrm{op}(y_1,\dots , y_n)
\]

\paragraph{Example}

Consider the theory for a monoid.
The signature for a monoid contains two operations, the binary multiplication $\cdot$
and the nullary operation defining the neutral element $e$.
For a given set of values $A$ the set $\mathrm{Tree}_{\mathrm{Mon}}(A)$ describes term trees
build using values from $A$, the constant $e$ and the operation $\cdot$.
The laws for a monoid state that the binary operations is associative, therefore
the equivalence relation $\sim_{\mathrm{Mon}}$ equates all trees, which are
equal up to rotation.
The constant $e$ is the left and right identity for $\cdot$, therefore
$\sim_{\mathrm{Mon}}$ also equates (sub)trees describing the terms $e\cdot t = t
= t \cdot e$.

Because trees can be rotated freely, the only important information is the order
of elements.
Because identities can be freely omitted, the only important term involving
identity is $e$.
It is easy to show that $\mathrm{Tree}_{\mathrm{Mon}}(A)
\;/\sim_{\mathrm{Mon}} = \mathrm{Free}_{\mathrm{Mon}}(A) \cong A^*$ i.e. the
free model for the theory of a monoid over a set of values $A$ are words over
$A$.
The empty word $\varepsilon$ corresponds to the identity element and $\cdot$
corresponds to concatenation $\circ$.
This is of course the usual definition of a free monoid over $A$.
% All other monoid with underlying set $A$ can be described as the image of a
% monoid-homomorphism from $A^*$.


\subsection{Effect Handler}
\label{preliminaries:handler}

Lastly we introduce the notion of an effect handler, again as described by
\textcite{DBLP:journals/corr/abs-1807-05923}.
Effect handlers  were first presented by \textcite{DBLP:conf/esop/PlotkinP09}.
They are a generalization of exception handlers and can be used to model a wide
range of non-algebraic effects.
A handler
\[
  h : \mathrm{Free}_{T}(V) \Rightarrow \mathrm{Free}_{T'}(V')
\]
is a map between free models for theories $T$ and $T'$ over value sets $V$ and
$V'$.
A handler is given by an additional $T$-model on $|\mathrm{Free}_{T'}(V')|$ and
a $T$-homomorphism from $\mathrm{Free}_{T}(V)$ to the additional model.
Notice that the set $|\mathrm{Free}_{T'}(V')|$ is equipped with two models, the
free $T'$ model it was generated by, as well as an additional (not necessarily
free) $T$ model.

Because a handler is a homomorphism from a free model it is uniquely determent
by a map $f : V \rightarrow |\mathrm{Free}_{T'}(V')|$ for the underlying
values and a map $h_i : P_i \times |\mathrm{Free}_{T'}(V')|^{A_i} \rightarrow
|\mathrm{Free}_{T'}(V')|$ for each operation $op_i$.
Furthermore, the handler satisfies the following two equations.
\begin{align*}
  h([\mathrm{pure}\;x]_{\sim_T}) &= f(x) \\
  h([\mathrm{op}_i(p,\kappa)]_{\sim_T}) &= h_i(p, h\circ\kappa )
\end{align*}
Usually when defining a handler the operations of the theory of the right-hand
side are a subset of the ones from the left-hand side.
The operations are interpreted by extending the structure on the values i.e.
mapping the impure model, given by $\mathrm{Free}_T(V)$, to a pure one.
For example, a handler for exceptions could be given by
\begin{align*}
  \mathrm{handleExc} &: \mathrm{Free}_{\text{Exc E}}(A) \Rightarrow
  \mathrm{Free}_\emptyset(E \uplus A) \cong E \uplus A \\
  \mathrm{pure}\;x&\mapsto \mathrm{pure}\; (inj_2\; x) \\
  \mathrm{throw}(e, \kappa)&\mapsto \mathrm{pure}\; (inj_1\; e) 
\end{align*}
where the source is a free model for the theory of exceptions $E$ and the target
is a free theory for the empty model.
The model for the theory of exceptions on the right-hand side is given by the
extended set of values, which corresponds to the usual monad modelling the
effect.

If the source and target theory share operations it is possible to interpret an
operations by itself i.e. not handling it.
This allow us to handle more complex, combined theories using multiple modular
handlers.
It is also possible to interpret operations using other operations i.e. to
simulate an effect in the source theory using effects in the target theory.
For example, one could simulate a logging effect (in Haskell \texttt{Writer String
  a}) using a state effect (in Haskell \texttt{State String a}).


\subsection{Free Monad}
\label{preliminaries:free-monad}

The syntax of an algebraic effect is described using the \textit{free monad}.
Given any functor \texttt{f} the data type \texttt{Free f a} is a monad.
The usual definition of the free monad in Haskell is given below.

\begin{minted}{haskell}
    data Free f a = Pure a | Impure (f (Free f a))

    instance (Functor f) => Monad (Free f) where
      return = Pure

      Pura x    >>= k = k x
      Impure fa >>= k = Impure (fmap (>>= k) fa)
\end{minted}
The free monad essential represents computation trees.
Computations are either \texttt{pure} i.e. they yield a value without calling
other operations or \texttt{impure}.
An \texttt{impure} computation is given by a value of the free monad, which was
lifted using the functor.
The constructors for \texttt{f a} correspond to operations and arguments of type
\texttt{a} correspond to subcomputations or arguments for the operation.
For example the functor \mintinline{haskell}{data Mon a = Mul a a | E}
corresponds to a signature with a binary operation and a constant.
\mintinline{haskell}{Free Mon a} is the type of computation trees using variable
of type \texttt{a}, the constant \texttt{E} and the binary operation
\texttt{Mul}.

\mintinline{haskell}{return} lifts a value to a computation by creating a
\mintinline{haskell}{Pure} computation which returns the value.
The \bind{} operation for the free monad corresponds to variable substitution.
When calling \bind{} on an \mintinline{haskell}{Impure} value, \bind{} is called
recursively on all subcomputations using \mintinline{haskell}{fmap} for the
functor.
\mintinline{haskell}{Pure} leafs are substituted with the subtree, generated
from there stored value.

The free monad corresponds to the free model for an algebraic theory $T$ without
equations i.e. the equivalence relation $\sim_T$ is just the identity relation on
$\mathrm{Tree}_T(\mathbb{X})$.
As a result, for theories with equation some trees which should be equal are
not.
As we will see later, this is not a problem, because when interpreting the
syntax using a handler different values can again be equated.
Therefore, if syntax is interpreted by a correct handler the equations should
hold again.


\paragraph{Free Monad in Agda}
When defining the free monad in Agda we cannot use an arbitrary functor as in
Haskell, because it would violate the strict positivity requirement.
Instead we will represent the functor as the extension of a container as
described in section \ref{container}.

\begin{code}
data Free (C : Container) (A : Set) : Set where
  pure    : A → Free C A
  impure  : ⟦ C ⟧ (Free C A) → Free C A
\end{code}
\AgdaDatatype{Container}s correspond to signatures.
The constructors for \AgdaField{Shape} correspond to the operation symbols
$\mathrm{op}_i$ and simultaneously store the parameter $P_i$.
A constructor without extra arguments corresponds to the an operation with
parameter set $\mathbb{1}$.
The type of positions for a shape corresponds to the arity set $A_i$.
The \AgdaField{Pos} type depends on a value of type \AgdaField{Shape} i.e. it
assigns an arity to each operation.
In the free model the parameters/child trees are given by the function $\kappa$,
which maps from the arity set to the carrier set.
This function corresponds to the position function in the
\AgdaDatatype{Container} extension, which maps from the type of positions to the
stored values.
In this case the stored values are exactly the subcomputations of type
\AgdaDatatype{Free}\AgdaSpace{}\AgdaArgument{C}\AgdaSpace{}\AgdaArgument{A}.

\paragraph{Algebraicity}
In there categorical formulation by \textcite{DBLP:conf/fossacs/PlotkinP02}
algebraic operations are required to have a certain natural condition.
When described by the free monad, algebraic operations are those which commute
with \AgdaFunction{>>=} i.e. for an $n$-ary algebraic operations the following
holds.

\[
  \mathbf{op}(x_1,\dots ,x_n) \bind{} k = \mathbf{op}(x_1\bind k,\dots
  ,x_n\bind k)
\]
For common operations with scopes, such as \texttt{once}, \texttt{catch} or
\texttt{fork} this does not hold.
Dealing with this limitation leads to the general notion of scoped effects.


\subsection{Scoped Effects}
\label{preliminaries:scoped-effects}

As shown by \textcite{DBLP:conf/fossacs/PlotkinP02} many computational effects
and therefore their associated monads can be described as algebraic theories,
but not all operations know from these monads are algebraic.
The most common example is \texttt{catch}, as it was noted early by
\textcite{DBLP:journals/acs/PlotkinP03}.
\texttt{catch} takes two arguments, a computation \texttt{p} which maybe
produces an exception and an handler \texttt{h}, which produces an alternate
result for each exception.
If \texttt{catch} were algebraic the following equation would hold.
\[
  \textbf{catch}\; p\; h \bind{k} = \textbf{catch}\;(p\bind{} k)\;(\lambda
  e.h\; e \bind{} k)
\]
The equations obviously does not hold, because if it would hold, \texttt{catch}
could not differentiate between an exception thrown inside or outside its scope.
The same pattern can be observed with other operations which delimit scopes.

As noted in section \ref{preliminaries:handler} non-algebraic and in particular
operations with scopes can be implemented as a handler.
\textcite{DBLP:conf/haskell/WuSH14} noted a general problem when working with
multiple handlers describing scoped operations.
The order of handlers determines the semantics of a program, but simultaneously
the handlers are used to delimit scopes.
Problems arise if the ordering for certain semantics and the forced positions
for delimiting scopes do not coincide.
It is either impossible to model certain semantics or one has to evaluate
handlers multiple times, reinjecting intermediate results into the
computation. % :/

To fix this problem \textcite{DBLP:conf/haskell/WuSH14} introduced two solutions
for syntactically delimiting scopes.
Both can be used to add operations with scopes to an implementation of algebraic
effects.
In chapter \ref{chapter:first-order} and \ref{chapter:higher-order} we discuss
how well these two approaches work in Agda.


% TODO: context of evaluating handler on subcomputations or change text in HO Exc
