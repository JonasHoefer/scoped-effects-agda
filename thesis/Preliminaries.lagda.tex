% ...

\section{Agda}

Agda\footnote{\url{https://github.com/agda/agda}} is a dependently typed
functional programming language.
The current version (2.6.1.1 as of Nov 2020) was originally developed by
\textcite{norell:thesis} under the name Agda2.
Due to its type system Agda can be used as a programming language and as a proof
assistant.

This section contains a short introduction to Agda, dependent types and the idea
of ``Propositions as types'' under which Agda can be used for theorem proving.

\subsection{Basic Syntax}

Agda's syntax is similar to Haskell's~\cite{Haskell98}.
Data types are declared with syntax similar to Haskell's GADTs.
Functions declarations and definitions are also similar to Haskell, except that
Agda uses a single colon for the typing relation.
In the following definition of \AgdaDatatype{ℕ}, \AgdaDatatype{Set} is the type
of all (small) types.

\begin{code}
data ℕ : Set where
  zero  : ℕ
  suc   : ℕ → ℕ
\end{code}
Ordinary function definitions are syntactically similar to Haskell.
Agda allows the definition of mixfix operators.
A mixfix operator is a nearly arbitrary list of symbols (builtin symbols like
colons are not allowed as part of operators).
An arbitrary number of underscores in the operator name are placeholders for
future parameters.
A mixfix operator can be applied partially by writing underscores for the
omitted parameters.

In the following definition of addition for natural numbers \AgdaFunction{+} is
a binary operator and therefore contains two underscores.

\begin{code}
_+_ : ℕ → ℕ → ℕ
zero   + m = m
suc n  + m = suc (n + m)
\end{code}
\begin{code}[hide]
{-# BUILTIN NATURAL ℕ #-}
\end{code}

\subsection{Dependent Types}
\label{preliminaries:dependent-types}
Agda has a stronger type system than Haskell.
Agda supports dependent types, i.e. types that can depended on values.
In this section we give a short theoretical introduction to dependent types and
show how they can be used to write programs in Agda.

The following type theoretic definitions are taken from the
book ``\citetitle{hottbook}'' ~\cite{hottbook}.
In type theory a type of types is called a universe.
Universes are usually denoted $\mathcal{U}$.
A function whose codomain is a universe is called a type family or a dependent
type.
$$
F : A \rightarrow \mathcal{U} \quad\text{where}\quad F(a) : \mathcal{U}
\quad\text{for}\quad a : A
$$
To avoid Russell's paradox, a hierarchy of universes $\mathcal{U}_1 :
\mathcal{U}_2 : \dots$ is introduced.
In Agda the universes are named \AgdaDatatype{Setₙ}, where \AgdaDatatype{Set₀}
can be abbreviated as \AgdaDatatype{Set}.
Usually, the universes are cumulative, i.e. if $\tau : \mathcal{U}_n$ then
$\tau : \mathcal{U}_k$ for $k>n$.
By default this is not the case in Agda.
Each type is member of a unique universe, but it is possible to lift a type to a
higher universe manually.
Since Agda 2.6.1 an experimental \texttt{--cumulativity} flag exists.

\paragraph{Dependent Function Types ($\Pi$-Types)} are a generalization of
function types.
The codomain of a $\Pi$-type is not fixed, but values with the argument the
function is applied to.
The codomain is defined using a type family of the domain, that specifies the
type of the result for each given argument.

$$
\prod_{a : A} B(a) \quad \text{with}\quad B : A \rightarrow\mathcal{U}
$$
An element of the above type is a function, that maps every $a : A$ to a $b :
B(a)$.
In Agda the builtin function type $\rightarrow$ is a $\Pi$-type.
An argument can be named by replacing the type $\tau$ with $x : \tau$, allowing
us to use the value as part of later types.

\paragraph{Dependent Pair Types ($\Sigma$-Types)} are a generalization of product
types.
A value of a $\Sigma$-type is a pair, but the type of the second component is
not fixed, but varies with the value of the first.
$$
\sum_{a : A} B(a) \quad \text{with}\quad B : A \rightarrow\mathcal{U}
$$
An element of the above type is a pair consisting of an $a : A$ and a $b : B(a)$.
In Agda \AgdaKeyword{record}s represent $n$-ary $\Sigma$-types.
Each field can be used in the type of the following fields.
The Agda standard library provides a data type called \AgdaDatatype{Σ}, that
represents normal dependent product types with two elements.
Furthermore, it provides a more convenient syntax for writing $\Sigma$-types.
The type above can be written as \AgdaDatatype{Σ[ a ∈ A ] (B a)}.

\paragraph{Programming with Dependent Types}

Next we introduce some common patterns for programming with dependent types.
The example of fixed length vectors and indexing using bounded subsets of
natural numbers is one of the examples presented by
\textcite{DBLP:conf/afp/Norell08}.

The data type \AgdaDatatype{Vec} depends on a type \AgdaDatatype{A} and a value
of type \AgdaDatatype{ℕ}.

\begin{code}
data Vec (A : Set) : ℕ → Set where
  _∷_  : {n : ℕ} → A → Vec A n → Vec A (suc n)
  []   : Vec A 0
\end{code}
Arguments on the left-hand side of the colon are called parameters and are the
same for all constructors.
Arguments on the right-hand side of the colon are called indices and can differ
for each constructor.
Therefore, \AgdaDatatype{Vec}\AgdaSpace{}\AgdaArgument{A} is a family of types
indexed by \AgdaDatatype{ℕ}.

The \AgdaInductiveConstructor{[]} constructor allows us to create an empty
vector of any type, but forces the index to be zero.
The \AgdaInductiveConstructor{\_∷\_} constructor appends an element to the front
of a vector of the same element type and increases the index by one.
Only these two constructors can be used to construct vectors.
Therefore, the index is always equal to the amount of elements stored in the
vector.

By encoding more information about data in its type we can add extra constraints
to functions working with it.
The following definition of \AgdaFunction{head} avoids error handling or
partiality by excluding the empty vector as a valid argument.

\begin{code}
head : ∀ {A n} → Vec A (suc n) → A
head (x ∷ _) = x
\end{code}
When pattern matching on the argument of \AgdaFunction{head} there is no case
for \AgdaInductiveConstructor{[]}.
The argument has type \AgdaDatatype{Vec A (suc n)} and
\AgdaInductiveConstructor{[]} has type \AgdaDatatype{Vec A 0}.
Those to types cannot be unified, because \AgdaInductiveConstructor{suc} and
\AgdaInductiveConstructor{zero} are different constructors of \AgdaDatatype{ℕ}.
Therefore, the \AgdaInductiveConstructor{[]} case does not apply.
By constraining the type of the function we were able to avoid the case, which
usually requires error handling or introduces partiality.

We can extend the idea of a type safe \AgdaFunction{head} function to type safe
indexing.
A vector of length \AgdaArgument{n} is indexed by the first \AgdaArgument{n}
natural numbers.
The type \AgdaDatatype{Fin}\AgdaSpace{}\AgdaArgument{n} represents the subset of
natural numbers smaller than $n$.

\begin{code}
data Fin : ℕ → Set where
  zero  : {n : ℕ} → Fin (suc n)
  suc   : {n : ℕ} → Fin n → Fin (suc n)
\end{code}
Because $0$ is smaller than every positive natural number,
\AgdaInductiveConstructor{zero} can only be used to construct an element of
\AgdaDatatype{Fin}\AgdaSpace{}$($\AgdaInductiveConstructor{suc}
\AgdaArgument{n}$)$ i.e. for every type except
\AgdaDatatype{Fin}\AgdaSpace{}\AgdaArgument{0}.

If any number is smaller than $n$, then its successor is smaller than $n+1$.
Therefore, if any number is an element of
\AgdaDatatype{Fin}\AgdaSpace{}\AgdaArgument{n}
then its successor is an element of
\AgdaDatatype{Fin}\AgdaSpace{}$($\AgdaInductiveConstructor{suc}\AgdaSpace{}\AgdaArgument{n}$)$.

We can construct a $k<n$ of type
\AgdaDatatype{Fin}\AgdaSpace{}\AgdaArgument{n} by starting with
\AgdaInductiveConstructor{zero} of type
\AgdaDatatype{Fin}\AgdaSpace{}$($\AgdaArgument{n - k}$)$ and applying
\AgdaInductiveConstructor{suc} $k$ times.
Using this definition of the bounded subsets of natural numbers we can define
a non partial version of \AgdaFunction{\_!\_} for vectors.

\begin{AgdaAlign}
\begin{code}
_!_ : ∀ {A n} → Vec A n → Fin n → A
(x  ∷ _ )  ! zero   = x
(_  ∷ xs)  ! suc i  = xs ! i
\end{code}
Notice that similar to \AgdaFunction{head} there is no case for
\AgdaInductiveConstructor{[]}.
\AgdaArgument{n} is used as index for
\AgdaDatatype{Vec}\AgdaSpace{}\AgdaArgument{A} and \AgdaDatatype{Fin}.
The constructors for \AgdaDatatype{Fin} only use \AgdaInductiveConstructor{suc}.
Therefore, the type
\AgdaDatatype{Fin}\AgdaSpace{}\AgdaInductiveConstructor{zero}
is not inhabited and the cases for \AgdaInductiveConstructor{[]} do not apply.

By case splitting on the vector first we could have obtained the term
\AgdaInductiveConstructor{[]}\AgdaSpace{}\AgdaFunction{!}\AgdaSpace{}\AgdaArgument{i}.
By case splitting on \AgdaArgument{i} we notice that no constructor for
\AgdaDatatype{Fin zero} exists.
Therefore, this case cannot occur, because the type of the argument is
uninhabited.
It is impossible to call the function, because we cannot construct an argument of
the correct type.
In this example we can either omit the case or explicitly state that the
argument is impossible to construct, by replacing it with $()$, allowing us to
omit the definition of the right-hand side of the equation.

\begin{code}
[]         ! () --impossible to reach case, no right-hand side
\end{code}
\end{AgdaAlign}
The other two cases are straightforward.
For index \AgdaInductiveConstructor{zero} we return the head of the vector.
For index \AgdaInductiveConstructor{suc}\AgdaSpace{}\AgdaArgument{i} we call
\AgdaFunction{\_!\_} recursively with the smaller index and the tail of the
vector.
Notice that the types for the recursive call change.
The tail of the vector \AgdaArgument{xs} and the smaller index \AgdaArgument{i}
are indexed over the predecessor of \AgdaArgument{n}.

As last example we implement the function \AgdaFunction{replicate}.
\begin{code}
replicate : ∀ {A} → A → (n : ℕ) → Vec A n
replicate a 0        = []
replicate a (suc n)  = a ∷ replicate a n
\end{code}
Given an element of type \AgdaArgument{A} and a natural number \AgdaArgument{n}
the function produces a vector of the given length.
Therefore, the function is a $\Pi$-type in its second argument.
The type of the returned value depends on the value of $n$.
By pattern matching on the given argument we obtain more information about the
return type.

The above, quite simple examples can also be implemented in Haskell, for
example using GADTs and type level natural numbers.
One of the advantages of Agda is the seamless and complete implementation of
dependent types.
For example, in contrast to a Haskell implementation we could simply reuse the
normal type of natural numbers to index other types.
The last example of the function replicate, which simply uses a $\Pi$-type is
more difficult to implement in Haskell, because it requires translating between
a value and type level number.
Later we will see more complex uses of dependent types, which cannot be
translated as easily to Haskell.


\subsection{Propositions as Types}

The idea of propositions as types, also know as Curry-Howard correspondence,
relates type theory to logic and is the basis for theorem proving in
dependently typed languages, like Agda.
In this section we give a short overview over the idea.
An more in depth explanation and an overview over the history of the idea can be
found in Wadler's paper of the same name \cite{DBLP:journals/cacm/Wadler15}.

As described by \textcite{DBLP:journals/cacm/Wadler15}, the idea of propositions
as types is a deep correspondents, relating \textit{propositions to types},
\textit{proofs to programs} and \textit{simplification of proofs to evaluation
of programs}.
First we will consider the connection between the simple typed lambda calculus
(with product and sum types) and \textit{intuitionistic} propositional logic.
The intuitionistic version of propositional logic uses weaker axioms than
classical propositional logic.
For example, the law of the excluded middle and double negation elimination do
not hold in general.
Intuitively speaking, in intuitionistic logic every proof has to be witnessed.
When proving a disjunction one has to state which of the alternatives holds i.e.
one cannot just prove existence.
With this view point in mind it should be clear why intuitionists refute the law
of the excluded middle, if $A \vee \neg A$ were true in general one would
already have to know which alternative holds.

A proposition corresponds to a type, while the elements of the type correspond
to the proofs of the proposition.
A proposition is true iff the type representing it is inhabited.
Constructing a proof for a proposition means constructing a value of its type.
Unit and bottom type correspond to true and false, because one can always deduce
true (construct a value of the unit type $\top$) and there exist no proof of
falsity i.e. $\bot$ the type representing false is not inhabited.
Under the above interpretation the usual logical connectives like $\wedge$,
$\vee$ and $\rightarrow$ correspond product, sum and function types.

For example, by the introduction rule for conjunctions, to construct a proof of
$A\wedge B$ one has to construct a proof of $A$ and a proof of $B$.
This corresponds to the construction of an element of the product type, because
to construct an element of $A\times B$ one has to provide an element of $A$ and
$B$.
The elimination rules for conjunction, i.e. one can deduce a proof of $A$ and a
proof of $B$ from the conjunction $A \wedge B$, corresponds to the projection
functions for products $\pi_1$ and $\pi_2$.
The argumentation for disjunction and sum/coproduct types is dual.

Functions from $A$ to $B$ map element of $A$ to elements of $B$ i.e. they
construct proofs of $B$ from proofs of $A$.
They correspond to implications.
Lambda abstracting of a value of type $A$ corresponds to assuming a proof of $A$
i.e. implication introduction, while function application corresponds to
implication elimination.

As explained in Section \ref{preliminaries:dependent-types}, Agda's type system
has dependent types.
With dependent types the construction of a type (and therefore whether the type
is inhabited) can change based on a value.
Therefore, dependent types correspond to predicates.
$\Sigma$ and $\Pi$ types correspond to existential and universal quantification.
To prove an existential one has to provide a value and a proof that the
proposition holds for this value.
These two correspond to the two elements of a the $\Sigma$ type.
The type of the second element depends on the first one, i.e. it is a proof for a
predicate on the first value.
To prove a universal quantified proposition one has to prove that it holds for
every possible value.
A $\Pi$ type is a function whose return type depends on the given value.
Given any value it returns a proof for the predicate on its argument.

Extending the type system with dependent types corresponds to extending
intuitionistic propositional logic to intuitionistic predicate logic.
Using the above corresponds Agda can be used to state and prove theorems in
this logic.
An overview over the correspondences between certain types and connectives, as
well as the corresponding syntax in Agda can be found in Table
\ref{preliminaries:propositions-as-types:table}.

\begin{table}
  \begin{center}
    \begin{tabular}{ccc}
      FOL & MLTT & Agda \\\hline
      $\forall x \in A: P(x)$ & $\Pi_{x:A}P(x)$ & \mintinline{agda}{(x : A) → P x} \\
      $\exists x \in A: P(x)$ & $\Sigma_{x:A} P(x)$ & \mintinline{agda}{Σ[ x ∈ A ] (P x)} \\
      $\neg A$ & $A \rightarrow \mathbb 0$ & \mintinline{agda}{A → ⊥} \\
      $P \wedge Q$ & $P \times Q$ & \mintinline{agda}{A × B} \\
      $P \vee Q$ & $P + Q$ & \mintinline{agda}{A ⊎ B} \\
      $P \rightarrow Q$ & $P \rightarrow Q$ & \mintinline{agda}{A → B} \\
      $\mathbf t$ & $\mathbb 1$ & \mintinline{agda}{⊤} \\
      $\mathbf f$ & $\mathbb 0$ & \mintinline{agda}{⊥}
    \end{tabular}
  \end{center}
  \label{preliminaries:propositions-as-types:table}
  \caption{Correspondence between first order logic and types in mathematical
    and Agda notation}
\end{table}

\subsection{Notions of Equality and Equality Types}

In the last section we saw how we can encode propositions from propositional and
predicate logic as types.
One of the most important proposition is equality, i.e. the proposition that
given two terms $a, b : A$ that $a$ and $b$ are equal.
When using Agda for theorem proving we have to express propositions like $a + b
= b + a$ and $2 = 1$, which could be true or false, to be able to prove or
disprove them.
In type theory and therefore in Agda we have to consider different notions of
equality.

When defining a program rule like
\AgdaFunction{truth}\AgdaSpace{}\AgdaSymbol{=}\AgdaSpace{}\AgdaNumber{42}
we are making an \textit{equality judgement}.
The symbol \AgdaFunction{truth} is \textit{definitionally equal} to
\AgdaNumber{42}.
A judgment is always true.
We define one term to be equal to another one and as result allow Agda to reduce
the left-hand side of the equality to the right-hand side.

The next notion of equality is \textit{computational equality}.
Two terms $t_1$ and $t_2$ are computationally equal if they reduce to the same
term.
For example, given the above definition of $+$ the terms $0+(0+n)$ and $n$ are
computationally equal, because using the first rule in the definition of plus
$0+(0+n)$ $\beta$-reduces to $n$.
On the other hand $n+0$ and $n$ are not computational equal, because for a free
variable $n$ none of the program rules for $+$ can be used to reduce the term
further.

To talk about the equality of two terms we have to use \textit{propositional
equality}.
We define a proposition representing the fact that two terms of type $A$ are
equal.
This proposition is usually encapsulated in an \textit{equality type} of
$A$~\cite{hottbook,DBLP:conf/afp/Norell08,DBLP:journals/scp/KokkeSW20}.

\begin{code}[hide]
infix 4 _≡_
\end{code}
\begin{code}
data _≡_ {A : Set} (x : A) : A → Set where
  refl : x ≡ x
\end{code}
The type
\AgdaArgument{x}\AgdaSpace{}\AgdaDatatype{≡}\AgdaSpace{}\AgdaArgument{y}
represents the proposition that $x$ and $y$ are equal.
The only way to construct evidence for the proposition is using the
\AgdaInductiveConstructor{refl} constructor, i.e. if $x$ and $y$ are actually
the same.

This notion of equality has the usually expected properties like transitivity,
symmetry and congruence.
The definition of \AgdaFunction{cong} shows the typical way of working with
equality proofs.

\begin{code}
cong : ∀ {A B x y} → (f : A → B) → x ≡ y → f x ≡ f y
cong f refl = refl
\end{code}
We expect that $≡$ is a congruence relation, i.e. if $x$ and $y$ are equal then
$f x$ and $f y$ are also equal.
We cannot produce a value of type
\AgdaArgument{f}\AgdaSpace{}\AgdaArgument{x}\AgdaSpace{}\AgdaDatatype{≡}\AgdaSpace{}\AgdaArgument{f}\AgdaSpace{}\AgdaArgument{y},
because the two are not equal, because $x$ and $y$ are arbitrary variables.
By pattern matching in the argument of type 
\AgdaArgument{x}\AgdaSpace{}\AgdaDatatype{≡}\AgdaSpace{}\AgdaArgument{y},
i.e. by inspecting the evidence that $x$ and $y$ are equal, we obtain more
information about the return type of the function.
Because \AgdaInductiveConstructor{refl} can only be constructed if the two
values are the same the two variables are unified.
Therefore, we have to produce a proof that $f x$ is equal to itself, which is
given by reflexivity.

By pattern matching on variables used in the equality type we can obtain more
information about the goal.
Either because the constructors them-self restrict the use of the variables or
because the terms used in the equality type can be reduced further.
Consider the following example.

\begin{code}
+-identʳ : ∀ n → n + 0 ≡ n
+-identʳ 0        = refl
+-identʳ (suc m)  = cong suc (+-identʳ m)
\end{code}
By pattern matching on the variable $n$ we obtain two cases, one for each
constructor (this is analogous to a proof by exhaustion).
In both cases the term
\AgdaArgument{n}\AgdaSpace{}\AgdaFunction{}\AgdaSpace{}\AgdaNumber{0}
can now be reduced further.
In the \AgdaNumber{0} case we obtain
\AgdaNumber{0}\AgdaSpace{}\AgdaFunction{+}\AgdaSpace{}\AgdaNumber{0}
on the left-hand side, which can be reduced to \AgdaNumber{0}, because the first
argument of \AgdaFunction{+} is now \AgdaNumber{0}.
The return type simplifies to 
\AgdaNumber{0}\AgdaSpace{}\AgdaDatatype{≡}\AgdaSpace{}\AgdaNumber{0}
for which we can simply construct evidence using
\AgdaInductiveConstructor{refl}.

The second case is more complex.
The left hand side still contains the free variable $m$, but reduces to
\AgdaInductiveConstructor{suc}\AgdaSpace{}$($\AgdaArgument{m}\AgdaSpace{}\AgdaFunction{+}\AgdaSpace{}\AgdaNumber{0}$)$
using the second rule for \AgdaFunction{\_+\_}.
Using a recursive call we obtain evidence for
\AgdaArgument{m}\AgdaSpace{}\AgdaFunction{+}\AgdaSpace{}\AgdaNumber{0}\AgdaSpace{}\AgdaDatatype{≡}\AgdaSpace{}\AgdaArgument{m}.
The recursive call, to obtain evidence for a smaller case, corresponds to the use
of the induction hypothesis in an inductive proof.
By applying \AgdaInductiveConstructor{suc} on both sides of the equality we
obtain a proof for the correct proposition.

We obtained a non-obvious equality by using just definitional and computational
equality together with the analogs of proofs by exhaustion and induction.
This proof can now be used in larger Proofs.
For example, using \AgdaFunction{cong} we can rewrite arbitrary terms containing
the left- or right-hand of the equality.

The above definition of propositional equality defines so called
\textit{intentional} propositional equality.
Intentional equality respects how objects are defined.
Extensional equality just observes how objects behave.
This distinction is important when comparing functions, as we will do in nearly
all later proofs.
Two functions which behave identical but are defined differently are
extensional but not intentional equal.
In Agda one can use extensional equality be arguing in another setoid (i.e.
using a different equivalence relation, which assumes that extensionality holds)
or by adding an axiom to modify the underlying theory.
For simplicity we will do the later and postulate the axiom of extensionality.

\begin{code}
postulate
  extensionality : ∀ {A : Set} {B : A → Set} 
    {f g : (x : A) → B x} → (∀ (x : A) → f x ≡ g x) → f ≡ g
\end{code}
The axiom simply states that the two functions \AgdaArgument{f} and
\AgdaArgument{g} are considered equal if they are equal point-wise.
It extends the intentional propositional equality to the extensional one, while
preserving Agda's consistency.


\subsection{Termination Checking}
\label{sec:termination-checking}

Under the idea of propositions as types, non-terminating functions correspond to
ill-formed.
For example, the term
\AgdaFunction{loop}\AgdaSpace{}$=$\AgdaSpace{}\AgdaFunction{loop} has an
arbitrary type, i.e. it should be a proof for any proposition.
Although, evaluating \AgdaFunction{loop} does not yield evidence for the
proposition, but a term without normal form or a circular argument.
Therefore, allowing the definition of non-terminating functions entails logical
inconsistency.
When defining functions Agda allows general recursion, but only terminating
functions are valid Agda programs.
Due to the undecidability of the halting problem Agda uses a heuristic
termination checker.
The termination checker proves termination by observing structural recursion.
Consider the following definitions of \AgdaDatatype{List} and
\AgdaFunction{map}.

\begin{code}
data List (A : Set) : Set where
  _∷_  : A → List A → List A
  []   : List A

map : {A B : Set} → (A → B) → (List A → List B)
map f (x ∷ xs)  = f x ∷ map f xs
map f []        = []
\end{code}
The \AgdaInductiveConstructor{[]} case does not contain a recursive call.
In the \AgdaInductiveConstructor{\_∷\_} case the recursive call to
\AgdaFunction{map} occurs on a structural smaller argument, i.e.
\AgdaArgument{xs} is a subterm of the argument
\AgdaArgument{x}\;\AgdaInductiveConstructor{∷}\;\AgdaArgument{xs}.
Because elements of \AgdaDatatype{List A} are finite the function
\AgdaFunction{map} terminates for every argument.


\subsubsection{Sized Types}
\label{preliminaries:sized-types}

In more complex recursive functions the structural recursion can be obscured.
For example, Agda does not allow inlining of functions containing pattern
matches during termination checking.
A common example are recursive calls in lambdas, that are passed to higher
order functions like \AgdaFunction{map} and \AgdaFunction{>>=}.
Consider the following definition of \mintinline{haskell}{mapRose}.
The recursive call is made by the higher-order function
\mintinline{haskell}{map} for lists.
An equivalent definition in Agda does not obviously terminate, because the
structural recursion is obscured.

\begin{minted}{haskell}
      data Rose a = Branch a [Rose a]

      mapRose :: (a -> b) -> Rose a -> Rose b
      mapRose f (Branch x rs) = Branch (f x) (map (mapRose f) rs)
\end{minted}
To still prove termination in such cases we can resort to type-based
termination checking as described by \textcite{DBLP:journals/lmcs/Abel08}.
The fundamental idea is to represent the size of a recursive data structure in
its type.
Termination of recursive functions can be proven by observing a reduction in
size on the type level.
Abel noted as main advantages robustness with respect to small or local changes
in code (which do not impact the type) as well as scaling to higher-order
functions and polymorphism~\cite{DBLP:journals/lmcs/Abel08}.
The latter will be essential in future chapters, since representations of
computational effects involve complex constructs, which obscure recursion on the
value level.

Type-based termination checking is available in Agda in the form of
\textit{sized types}.
The special, builtin type \AgdaDatatype{Size} can be used to annotate data types
with a value which represents an upper bound on the size of the types values.
A set of builtin functions and types can be used to establish relations between
different sizes and therefore assist the termination checker.
The elements of \AgdaDatatype{Size} are well ordered.
\AgdaFunction{↑\_} is the successor operation on \AgdaDatatype{Size}, i.e. 
\AgdaArgument{i}$\;<\;$\AgdaFunction{↑}\AgdaSpace{}\AgdaArgument{i}.
\AgdaFunction{\_⊔ˢ\_} is the supremum with respect to the relation.
\AgdaInductiveConstructor{∞} is the top element in \AgdaDatatype{Size}.
Given a \AgdaDatatype{Size} \AgdaArgument{i}, the type of all smaller sizes can
be constructed using the \AgdaDatatype{Size<\_} family of types\footnote{We will
  not need this construction. It is usually used when working with
  \AgdaKeyword{coinductive} data types.}.
If the size decreases across all recursive calls of a function, termination
follows from the well-ordering on \AgdaDatatype{Size}.

\begin{code}[hide]
open import Agda.Builtin.Size public
  renaming ( SizeU to SizeUniv )  --  sort SizeUniv
  using    ( Size                 --  Size   : SizeUniv
           ; Size<_               --  Size<_ : Size → SizeUniv
           ; ↑_                   --  ↑_     : Size → Size
           ; _⊔ˢ_                 --  _⊔ˢ_   : Size → Size → Size
           ; ∞ )                  --  ∞      : Size
\end{code}

A common idiom for data types is to mark all non-inductive constructors and
recursive occurrences with an arbitrary size \AgdaArgument{i} and all inductive
constructors with the next larger size
\AgdaFunction{↑}\AgdaSpace{}\AgdaArgument{i}.
The size therefore corresponds to the height of the tree described by the term
($+$ the initial height for the lowest non-inductive constructor).

Let us again consider the rose tree example.
The size index can be intuitively thought of as the height of the tree.

\begin{code}
data Rose (A : Set) : Size → Set where 
  rose : {i : Size} → A → List (Rose A i) → Rose A (↑ i)
\end{code}
When \AgdaFunction{mapRose} is defined in terms of
\AgdaFunction{map} the termination is obscured.
The argument of the functions passed to \AgdaFunction{map} is not recognized as
structurally smaller than the given rose tree.
Using size annotations we can fix this problem.

\begin{code}
mapRose : {A B : Set} {i : Size} → (A → B) → (Rose A i → Rose B i)
mapRose f (rose x xs) = rose (f x) (map (mapRose f) xs)
\end{code}
By pattern matching on the argument of type
\AgdaDatatype{Rose}\AgdaSpace{}\AgdaArgument{A} of size
\AgdaFunction{↑}\AgdaSpace{}\AgdaArgument{i} we obtain a \AgdaDatatype{List} of
trees of size \AgdaArgument{i}.
The recursive calls via \AgdaFunction{map} therefore occur on tree of type 
\AgdaDatatype{Rose}\AgdaSpace{}\AgdaArgument{A}\AgdaSpace{}$($\AgdaFunction{↑}\AgdaSpace{}\AgdaArgument{i}$)$.
Therefore, the functions has to terminate.

In this case inlining the call of \AgdaFunction{map} also solves the termination
problem.
When generalizing the definition of the tree from \AgdaDatatype{List} to an
arbitrary functor this would not be possible.
In many cases inlining all helper functions is either not feasible or would lead
to large and unreadable programs.


\subsection{Strict Positivity}

In a type system with arbitrary recursive types, it is possible to to implement a
fixpoint combinator and therefore non-terminating functions without explicit
recursion.
As explained in Section \ref{sec:termination-checking} this entails logical
inconsistency.
Agda allows only strictly positive data types.
A data type $D$ is strictly positive if all constructors are of the form
$$
  A_1 \rightarrow A_2 \rightarrow \dots \rightarrow A_n \rightarrow D
$$
where each $A_i$ does not mention $D$ or is of the form
$$
B_1 \rightarrow B_2 \rightarrow \dots \rightarrow B_n \rightarrow D
$$
where $B_j$ does not mention $D$.
By restricting recursive occurrences of a data type in its definition to strict
positive positions strong normalization is
preserved~\cite{Wadler1991RecursiveTF}.


\subsubsection{Container}
\label{container}

Because of the strict positivity requirement it is not allowed to apply generic
type constructors to recursive occurrences of a data type in its definition.
The reason for this restriction is that a type constructor is not required to
use its argument only in strictly positive positions.
To still work generically with type constructors or more precise functors we
need a more restrictive representation, which only uses its argument in a
strictly positive position.
One representation of such strictly positive functors are containers.

Containers are generic representations of data types, that store values of an
arbitrary type.
They were introduced by \textcite{DBLP:conf/fossacs/AbbottAG03}.
A container is defined by a type of shapes $S$ and a type of positions for each
of its shapes $P : S \rightarrow \mathcal{U}$.
Usually containers are denoted $S \rhd P$.
A common example are lists.
The shape of a list is given by its length, therefore the shape type is
$\mathbb{N}$.
A list of length $n$ has exactly $n$ places or positions containing data.
Therefore, the type of positions is $\Pi_{n : \mathbb{N}}\mathrm{Fin}\;n$ where
$\mathrm{Fin}\;n$ is the type of natural numbers smaller than $n$.
The extension of a container is a functor $\lBrack S \rhd P \rBrack $,
whose lifting of types is given by

$$
  \lBrack S \rhd P \rBrack\;X = \sum\limits_{s : S} P s \rightarrow X.
$$
A lifted type corresponds to the container storing elements of the given type
e.g. $\lBrack\, \mathbb{N} \rhd \mathrm{Fin}\;\rBrack \; A \cong
\mathrm{List}\;A$.
The second element of the dependent pair sometimes called position function.
It assigns each position a stored value.
The functors action on functions is given by

$$
  \lBrack S \rhd P \rBrack \;f\;\langle s , pf \rangle = \langle s , f \circ pf \rangle.
$$
We can translate these definition directly to Agda.
Instead of a \AgdaKeyword{data} declaration we can use \AgdaKeyword{record}
declarations.
Similar to other languages \AgdaKeyword{record}s are pure product types.
A \AgdaKeyword{record} in Agda is an $n$-ary dependent product type, i.e. the
type of each field can contain all previous values.

\begin{code}
record Container : Set₁ where
  constructor _▷_
  field
    Shape : Set
    Pos : Shape → Set
open Container
\end{code}
As expected, a container consists of a type of shapes and a dependent type of
positions.
Notice that \AgdaDatatype{Container} is an element of \AgdaDatatype{Set₁},
because it contains a type from \AgdaDatatype{Set} and therefore has to be
larger.
Next we define the lifting of types, i.e. the container extension, as a function
between universes.

\begin{code}[hide]
open import Data.Product using (Σ-syntax; _,_) renaming (proj₁ to π₁; proj₂ to π₂)
open import Function using (_∘_)
\end{code}
\begin{code}
⟦_⟧ : Container → Set → Set
⟦ S ▷ P ⟧ A = Σ[ s ∈ S ] (P s → A)
\end{code}
Using this definition we can define \AgdaFunction{fmap} for containers.

\begin{code}
fmap : ∀ {A B C} → (A → B) → (⟦ C ⟧ A → ⟦ C ⟧ B)
fmap f (s , pf) = (s , f ∘ pf)
\end{code}
As mentioned in Section \ref{sec:termination-checking}, Agda does not inline
functions containing a pattern match, even if the \AgdaDatatype{INLINE} pragma
is used.
Because container extensions are just \AgdaKeyword{record} types, we can define
\AgdaFunction{fmap} slightly different to assist the termination checker.
By using the projection functions to access the components of the $\Sigma$-type
we can avoid the pattern match.
By forcing Agda to inline the function using the pragma we define a version of
\AgdaFunction{fmap}, which is transparent for the termination checker.
This allows us to use this small auxiliary function in recursive functions.

\begin{code}
fmapᴵ : ∀ {A B C} → (A → B) → (⟦ C ⟧ A → ⟦ C ⟧ B)
fmapᴵ f c = π₁ c , f ∘ π₂ c
{-# INLINE fmapᴵ #-}
\end{code}


\section{Curry}
\label{preliminaries:curry}

Curry \cite{Hanus95curry} is a functional logic programming language.
It combines paradigms from functional programming languages like Haskell
with those from logical languages like Prolog.
Curry is based on Haskell.
Its syntax and semantics not involving nondeterminism closely resemble Haskell.
Curry integrates logical features, such as nondeterminism and free variables
with a few additional concepts.

Like in Haskell, functions are defined using equations, but Curry assigns
different semantics to overlapping clauses.
When calling a function all right-hand sides of matching left-hand sides are
executed.
This introduces nondeterminism.
Nondeterminism is integrated as an \textit{ambient effect}, i.e. the effect is
not represented at type level.
The simplest example of a nondeterministic function is the choice operator
\texttt{?}.

\begin{minted}{haskell}
     (?) :: A -> A -> A
     x ? _ = x
     _ ? y = y
\end{minted}
Both equations always match.
Therefore, \texttt{?} introduces a nondeterministic choice between its two
arguments.
Using \texttt{?} we can define a simple nondeterministic program.

\begin{minted}{haskell}
     coin :: Int
     coin = 0 ? 1

     twoCoins :: Int
     twoCoins = coin + coin
\end{minted}
\texttt{coin} chooses non-deterministically between $0$ and $1$.
Executing \texttt{coin} therefore yields these two results.
When executing \texttt{twoCoins} the two calls of \texttt{coin} are independent.
Both choose between $0$ and $1$, therefore \texttt{twoCoins} yields the results
$0$, $1$, $1$ and $2$.

\subsection{Call-Time Choice}
\label{call-time-choice}
Next we take a look at the interactions between nondeterminism and function
calls.

\begin{minted}{haskell}
     double :: Int -> Int
     double x = x + x
     
     doubleCoin :: Int
     doubleCoin = double coin
\end{minted}
When calling \texttt{double} with a nondeterministic value two behaviors are
conceivable.
The first possibility is that the choice is moved into the function, i.e. both
\texttt{x} choose independent of each other, yielding the results $0$, $1$, $1$
and $2$.
The second possibility is choosing a value before calling the function and
choosing between the results for each possible argument.
In this case both \texttt{x} have the same value, therefore the possible results
are $0$ and $2$.
The latter option is called call-time choice and it is the one implemented by
Curry\footnote{
Similar to Haskell, these semantics provide referential transparency (one
can freely replace a term with its definition) and definiteness (each variable has
exactly one value), but not unfoldability~\cite{DBLP:journals/acta/SondergaardS89}.
That is, one cannot simply substitute in non-value function arguments, because
they could contain choices.
Instead one expects the above behavior of conceptually choosing before calling
the function.}.
The rational behind call-time choice is that deterministic functions behave the
same for deterministic and non-deterministic inputs.
A variable bound by a lambda or \texttt{let} always refers to just a single
value.

Similar to Haskell, Curry programs are evaluated lazily.
The evaluation of an expression is delayed until its result is demanded and each
expression is evaluated at most once.
The later is important when expressions are named and reused via \texttt{let}
bindings or lambda abstraction.
The named expression is evaluated the first time it is needed.
If the result is needed again the old value is reused.
This behavior is called sharing.
Usually function application is defined using the \texttt{let}
primitive~\cite{DBLP:conf/popl/Launchbury93}.
Applying a non-variable expression to a function introduces a new intermediate
result, which is bound using \texttt{let}.
$$
(\lambda x.\sigma) \tau = \texttt{let}\;y = \tau\;\texttt{in}\;\sigma[x\mapsto y]
$$
As noted by \textcite{DBLP:journals/corr/abs-2008-11999}, in Haskell sharing is
used to evaluate programs more efficiently, but in Curry sharing is needed for
correct evaluation.
Sharing allows delaying the evaluation of a function arguments as well as
linking all their uses.
Sharing a nondeterminism computations means making the same choices in all
places the shared term is used.
If the choice is demanded, the resulting behavior is identical to choosing
before calling the function.
Furthermore, because the \texttt{let} primitive introduces sharing, we expect a
variable bound by a \texttt{let} to behave similar to one bound by a function.
\begin{minted}{haskell}
  sumCoin :: Int
  sumCoin = let x = coin in x + x
\end{minted}
As expected this function yields the results $0$ and $2$.

Because the program is evaluated lazily, it is possible to never demanded a choice.
In this case the number of branches does not increase.
Consider the following example of the length and the sum of a nondeterministic list.
\begin{minted}{haskell}
     length :: [a] -> Int
     length []     = 0
     length (_:xs) = 1 + length xs

     sum :: [Int] -> Int
     sum []     = 0
     sum (x:xs) = x + sum xs

     coins :: [Int]
     coins = [coin, coin]
\end{minted}
When summing up the list \texttt{coins}, all choices in the components are
demanded.
Therefore, we obtain the four results $0$, $1$, $1$ and $2$.
In case of length this does not happen.
The function does not demand the values stored in the list.
Therefore, we obtain just the single result $2$.


\section{Algebraic Computational Effects}

Computational effects are a wide concept and usually introduced by example.
They include I/O, exceptions, nondeterminism, state and
continuations~\cite{DBLP:conf/fossacs/PlotkinP02,DBLP:journals/corr/abs-1807-05923}.
Algebraic effects are computational effects, that can be described using an
algebraic theory.
They were first presented \textcite{DBLP:conf/fossacs/PlotkinP02}, as an
alternative to monads for describing computational effects.
They capture a wide class of computational effects.
% Examples include all of the above, except general continuations.
Due to their uniform representation they provide a general way of defining,
combining and reasoning about computational
effects~\cite{DBLP:journals/corr/PlotkinP13}.
Furthermore, by describing computational effects in terms of operations and
equations, they directly allow equational reasoning about effectful programs.

Handlers for algebraic effects were first presented by
\citeauthor{DBLP:conf/esop/PlotkinP09} as a generalization of exception
handlers~\cite{DBLP:conf/esop/PlotkinP09}.
They describe how to transform effectual computations and can be used to
describe non-algebraic operations, that is operations which did not fit in the
general theory of algebraic effects.
Examples include operations with scopes such as \texttt{catch} and
\texttt{once}.

\textcite{DBLP:conf/haskell/WuSH14} describe a problem with this approach when
multiple handlers interact.
The ordering of handlers induces a semantic for the program, but simultaneously
handlers which implement scoping operations also delimit scopes.
The correct ordering for scoping and semantics maybe not coincide.
Wu et al. fix this problem by introducing new syntactic constructs to delimit
scopes, removing the responsibility from the
handler~\cite{DBLP:conf/haskell/WuSH14,DBLP:conf/lics/PirogSWJ18}.

In the following sections we give a short introduction to the classical notion
of algebraic effects and handlers as described by
\textcite{DBLP:journals/corr/abs-1807-05923} and used in Chapter
\ref{chapter:first-order} as well as scoped effects as described by
\textcite{DBLP:conf/haskell/WuSH14}.

\subsection{Algebraic Theories}

The following definition is similar\footnote{We use a slightly different
  notation for the assignment of variables when interpreting terms.} to the one
given by \textcite{DBLP:journals/corr/abs-1807-05923}.
Bauer uses abstract algebra not arbitrary categories.
We use his work to define some basic concepts and terminology regarding
algebraic effects.

An algebraic theory consists of a signature describing the syntax of the
operations together with a set of equations.
A signature is a set of operation symbols together with an arity set and an
additional parameter.
Operations of this form are usually denoted with a colon and $\leadsto$
between the three sets, suggesting that they describe special functions.
\[
  \Sigma = \{ \mathrm{op}_i : P_i \leadsto A_i  \}
\]
Given a signature $\Sigma$ we can build terms over a set of variables $\mathbb{X}$.
\[
  x \in \mathrm{Term}_\Sigma(\mathbb{X}) \quad\text{for}\quad x\in\mathbb{X} \qquad
  \mathrm{op}_i(p, \kappa) \in \mathrm{Term}_\Sigma(\mathbb{X}) \quad\text{for}\quad
  p\in P_i,\;\kappa : A_i \rightarrow \mathrm{Term}_\Sigma(\mathbb{X})
\]
Terms can be used to form equations of the form $x\;|\;l=r$.
Each equation consists of two terms $l$ and $r$ over a set of variables $x$.
A signature $\Sigma_T$ together with a set of equations $\mathcal{E}_T$ forms an
algebraic theory $T$.
\[
  T = (\Sigma_T, \mathcal{E}_T)
\]
An interpretation $I$ of a signature is given by a carrier set $|I|$ and an
interpretation for each operation, given by $\lBrack\cdot\rBrack_I$.
An interpretation of an operation $\mathrm{op}_i$ is given by a functions to the
carrier set $|I|$, that takes an additional parameter from $P_i$ and $|A_i|$
parameters as a function from the arity set to the carrier set $|I|$.
\[
  \lBrack \mathrm{op}_i \rBrack_I : P_i \times |I|^{A_i} \rightarrow |I|
\]
Given a function $\iota : \mathbb{X} \rightarrow |I|$, assigning each variable
a value, we can give an interpretation for terms $\lBrack\cdot\rBrack_{(I,\iota)}$.
\begin{align*} \lBrack x \rBrack_{(I,\iota)} &= \iota(x) \\
  \lBrack \mathrm{op}_i(p, \kappa)\rBrack_{(I,\iota)} &= \lBrack op_i \rBrack_I (p, \lambda p. \lBrack\kappa(p)\rBrack_{(I, \iota)})
\end{align*}
An interpretation for $T$ is called $T$-model if it validates all equations in $\mathcal{E}_T$.

% \paragraph{Algebraic Theories for Computational Effects}
% 
% \begin{itemize}
%   \item Effects which can be described using an algebraic theory
%   \item evaluation order from top to bottom
%   \item interpretation of operations essential performs side effect
%   \item $|I|^{A_i}$ corresponds to continuation, i.e. rest of the computation
%     processing the result
%   \item if $A_i$ is infinite (e.g. for \texttt{get} it is $S$) it describes possible
%     continuations for each state (contrast: in math operations usually process
%     all parameters $\Leftrightarrow$ results of subcomputations)
% \end{itemize}

\subsubsection{Free Model}

For each algebraic theory $T$ we can generate a so called free model
$\mathrm{Free}_T(\mathbb{X})$.
The free model is the optimal model for the theory, i.e. it validates just
equations from $\mathcal{E}_T$ and the ones directly following from them and no
more.

The set $\mathrm{Tree}_T(\mathbb{X})$ contains term trees, built from the
variables in $\mathbb{X}$ and the operations of the signature $\Sigma_T$.
\[
  \mathrm{pure}\;x \in \mathrm{Tree}_T(\mathbb{X}) \qquad \mathrm{op}_i(p,\kappa)
  \in \mathrm{Tree}_T(\mathbb{X}) \quad\text{for}\quad p\in P_i \quad \kappa :
  A_i \rightarrow \mathrm{Tree}_T(\mathbb{X})
\]
The free model for $T$ is given by

\[
  \mathrm{Free}_T(\mathbb{X}) = \mathrm{Tree}_T(\mathbb{X})\; /\;\sim_T
\]
where $\sim_T$ is the least equivalence relation, such that for all equations
$x\;|\;l=r\in\mathcal{E}_T$, $l\sim_T r$.
Furthermore $\sim_T$ is required to be a congruence relation for the operations
of the signature, i.e.
\[
  \forall i\in\{1,\dots ,n\}. x_i\sim_T y_i \Rightarrow \mathrm{op}(x_1,\dots , x_n)
  \sim_T \mathrm{op}(y_1,\dots , y_n)
\]

\paragraph{Example}

Consider the theory for a monoid.
The signature for a monoid contains two operations, the binary multiplication $\cdot$
and the nullary operation defining the neutral element $e$.
For a given set of values $A$ the set $\mathrm{Tree}_{\mathrm{Mon}}(A)$ describes term trees
build using values from $A$, the constant $e$ and the operation $\cdot$.
The laws for a monoid state that the binary operations is associative, therefore
the equivalence relation $\sim_{\mathrm{Mon}}$ equates all trees, which are
equal up to rotation.
The constant $e$ is the left and right identity for $\cdot$, therefore
$\sim_{\mathrm{Mon}}$ also equates (sub)trees describing the terms $e\cdot t = t
= t \cdot e$.

Because trees can be rotated freely, the only important information is the order
of elements.
Because identities can be freely omitted, the only important term involving
identity is $e$.
It is easy to show that $\mathrm{Tree}_{\mathrm{Mon}}(A)
\;/\sim_{\mathrm{Mon}} = \mathrm{Free}_{\mathrm{Mon}}(A) \cong A^*$, i.e. the
free model for the theory of a monoid over a set of values $A$ are words over
$A$.
The empty word $\varepsilon$ corresponds to the identity element and $\cdot$
corresponds to concatenation $\circ$.
This is of course the usual definition of a free monoid over $A$.
% All other monoid with underlying set $A$ can be described as the image of a
% monoid-homomorphism from $A^*$.


\subsection{Effect Handlers}
\label{preliminaries:handler}

Lastly we introduce the notion of an effect handler, again as described by
\textcite{DBLP:journals/corr/abs-1807-05923}.
Effect handlers were first presented by \textcite{DBLP:conf/esop/PlotkinP09}.
They are a generalization of exception handlers and can be used to model a wide
range of non-algebraic effects.
A handler
\[
  h : \mathrm{Free}_{T}(V) \Rightarrow \mathrm{Free}_{T'}(V')
\]
is a map between free models for theories $T$ and $T'$ over value sets $V$ and
$V'$.
A handler is given by an additional $T$-model on $|\mathrm{Free}_{T'}(V')|$ and
a $T$-homomorphism from $\mathrm{Free}_{T}(V)$ to the additional model.
Notice that the set $|\mathrm{Free}_{T'}(V')|$ is equipped with two models, the
free $T'$ model it was generated by, as well as an additional (not necessarily
free) $T$ model.

Because a handler is a homomorphism from a free model it is uniquely determent
by a map $f : V \rightarrow |\mathrm{Free}_{T'}(V')|$ for the underlying
values and the interpretations $\lBrack op_i \rBrack : P_i \times
|\mathrm{Free}_{T'}(V')|^{A_i} \rightarrow |\mathrm{Free}_{T'}(V')|$ for each
operation $op_i$ in the target $T$-model.
The handler is then given by the following equations.
\begin{align*}
  h([\mathrm{pure}\;x]_{\sim_T}) &= f(x) \\
  h([\mathrm{op}_i(p,\kappa)]_{\sim_T}) &= h_i(p, h\circ\kappa )
\end{align*}
Usually when defining a handler the operations of the theory of the right-hand
side are a subset of the ones from the left-hand side.
The operations are interpreted by extending the structure on the values, i.e.
mapping the impure model, given by $\mathrm{Free}_T(V)$, to a pure one.
For example, a handler for exceptions could be given by
\begin{align*}
  \mathrm{handleExc} &: \mathrm{Free}_{\text{Exc E}}(A) \Rightarrow
  \mathrm{Free}_\emptyset(E \uplus A) \cong E \uplus A \\
  \mathrm{pure}\;x&\mapsto \mathrm{pure}\; (inj_2\; x) \\
  \mathrm{throw}(e, \kappa)&\mapsto \mathrm{pure}\; (inj_1\; e) 
\end{align*}
where the source is a free model for the theory of exceptions $E$ and the target
is a free model for the empty theory.
The model for the theory of exceptions on the right-hand side is given by the
extended set of values, which corresponds to the usual monad modelling the
effect.
Another handler for the theory of exceptions is the $\mathrm{catch}$ operation.
The handler is parameterized of the function mapping exceptions to alternative
results.
The handler simply maps occurrences of $\mathrm{throw}\;i$ to the $\mathrm{pure}$
alternative result for $e$.

If the source and target theory share operations it is possible to interpret an
operations by itself, i.e. not handle it.
This allow us to handle more complex, combined theories using multiple modular
handlers.
It is also possible to interpret operations using other operations, i.e. to
simulate an effect in the source theory using effects in the target theory.
For example, one could simulate a logging effect (in Haskell \texttt{Writer String
  a}) using a state effect (in Haskell \texttt{State String a}).


\subsection{Free Monads}
\label{preliminaries:free-monad}

The syntax of an algebraic effect is described using the \textit{free
monad}~\cite{DBLP:journals/jfp/Swierstra08}.
Given any functor \texttt{f} the data type \texttt{Free f a} is a monad.
Furthermore, the natural transformations from any functor $f$ to any monad $m$
(in Haskell functions of type \texttt{forall a. f a -> m a}) are in bijective
correspondence with monad homomorphisms from \texttt{Free f a} to \texttt{m}.
Therefore, each monad can be described as the image of a monad homomorphisms
from a free monad.
The usual definition of the free monad in Haskell is given below.

\begin{minted}{haskell}
    data Free f a = Pure a | Impure (f (Free f a))

    instance (Functor f) => Monad (Free f) where
      return = Pure

      Pura x    >>= k = k x
      Impure fa >>= k = Impure (fmap (>>= k) fa)
\end{minted}
The free monad essential represents computation trees.
Computations are either \texttt{pure}, they yield a value without calling
other operations, or \texttt{impure}.
An \texttt{impure} computation is given by a value of the free monad, which was
lifted using the functor.
The constructors for \texttt{f a} correspond to operations and arguments of type
\texttt{a} correspond to subcomputations or arguments for the operation.
For example, the functor \mintinline{haskell}{data Mon a = Mul a a | E}
corresponds to a signature with a binary operation and a constant.
\mintinline{haskell}{Free Mon a} is the type of computation trees using variable
of type \texttt{a}, the constant \texttt{E} and the binary operation
\texttt{Mul}.

\mintinline{haskell}{return} lifts a value to a computation by creating a
\mintinline{haskell}{Pure} computation which returns the value.
The \bind{} operation for the free monad corresponds to variable substitution.
When calling \bind{} on an \mintinline{haskell}{Impure} value, \bind{} is called
recursively on all subcomputations using \mintinline{haskell}{fmap} for the
functor.
\mintinline{haskell}{Pure} leafs are substituted with the subtree, generated
from their stored value.
Therefore, \bind{} allows processing the resulting values of an effectful
program with another effectful program.

The free monad corresponds to the free model for an algebraic theory $T$ without
equations, i.e. the equivalence relation $\sim_T$ is just the identity relation on
$\mathrm{Tree}_T(\mathbb{X})$.
When implementing algebraic effect one simply uses theories without equations,
that is, just the signatures.
By moving from free models to term trees we obtain a uniform representation of
syntax as free monad over a functor.
As a result, for theories with equation some trees which should be equal are
not.
These terms are just syntax without semantics\footnote{For some effects these
  two are the same, i.e. their corresponding monad is free.
  For example, in case of exceptions the \texttt{throw} already has the correct
  semantics.
  Because it has no sub-computations, it aborts the program.}.
Semantics are completely determent by the handler, that interprets the syntax
by mapping it to an actual model.
When interpreting the syntax using a handler falsely different values can again
be equated, by treating them the identically.
Therefore, if syntax is interpreted by a correct handler the equations should
hold again.
Therefore, working directly on trees and not equivalence classes of trees is
correct as long as the handlers are correct.
Furthermore, by representing effectful programs as computations trees one delays
the actual evaluation.
This allows a clear separation into syntax and semantics and allows choosing the
semantics at a later point in time.

\paragraph{Free Monads in Agda}
When defining the free monad in Agda we cannot use an arbitrary functor as in
Haskell, because it would violate the strict positivity requirement.
Instead we will represent the functor as the extension of a container as
described in Section \ref{container}.
Replacing arbitrary functors with container extension, especially to implement
the free monad, is a well known
approach~\cite{DBLP:journals/programming/DylusCT19,DBLP:conf/haskell/ChristiansenDB19,DBLP:conf/mpc/McBride15}.

\begin{code}
data Free (C : Container) (A : Set) : Set where
  pure    : A → Free C A
  impure  : ⟦ C ⟧ (Free C A) → Free C A
\end{code}
Because we constrain the generator from arbitrary to strictly positive functors,
we cannot model all free monads with this data type.
Using \AgdaDatatype{Free} we can model a class of free monads, which are
accepted by Agda.
% Good restriction e.g. Cont is a weird functor to describe syntax
% corresponds to freer monad?
The definition of \AgdaDatatype{Container}s correspond closely to the one for
signatures.
The constructors for \AgdaField{Shape} correspond to the operation symbols
$\mathrm{op}_i$ and the constructor arguments correspond to the parameter $P_i$.
A constructor without extra arguments corresponds to the an operation with
parameter set $\mathbb{1}$.
The type of positions for a shape corresponds to the arity set $A_i$.
The \AgdaField{Pos} type depends on a value of type \AgdaField{Shape} i.e. it
assigns an arity to each operation.
In the free model the parameters/child trees are given by the function $\kappa$,
which maps from the arity set to the carrier set.
This function corresponds to the position function in the
\AgdaDatatype{Container} extension, which maps from the type of positions to the
stored values.
In this case the stored values are exactly the subcomputations of type
\AgdaDatatype{Free}\AgdaSpace{}\AgdaArgument{C}\AgdaSpace{}\AgdaArgument{A}.

\paragraph{Algebraicity}
In there categorical formulation by \textcite{DBLP:conf/fossacs/PlotkinP02}
algebraic operations are required to have a certain natural condition.
In the earlier definition this requirement is captured by restricting the shape
of equations.
When described by the free monad, algebraic operations are those which commute
with \bind{}, i.e. for an $n$-ary algebraic operations the following
holds~\cite{DBLP:conf/lics/PirogSWJ18}.

\[
  \mathbf{op}(x_1,\dots ,x_n) \bind{} k = \mathbf{op}(x_1\bind k,\dots
  ,x_n\bind k)
\]
For common operations with scopes, such as \texttt{once}, \texttt{catch} or
\texttt{fork}, this does not hold.
Dealing with this limitation leads to the general notion of scoped effects.


\subsection{Scoped Effects}
\label{preliminaries:scoped-effects}

As shown by \textcite{DBLP:conf/fossacs/PlotkinP02} many computational effects
and therefore their associated monads can be described as algebraic theories,
but not all operations know from these monads are algebraic.
The most common example is \texttt{catch}, as it was noted early by
\textcite{DBLP:journals/acs/PlotkinP03}.
\texttt{catch} takes two arguments, a computation \texttt{p} which maybe
produces an exception and an handler \texttt{h}, which produces an alternate
result for each exception.
If \texttt{catch} were algebraic the following equation would hold.
\[
  \textbf{catch}\; p\; h \bind{k} = \textbf{catch}\;(p\bind{} k)\;(\lambda
  e.h\; e \bind{} k)
\]
The equations obviously does not hold, because if it would hold, \texttt{catch}
could not differentiate between an exception thrown inside or outside its scope.
The same pattern can be observed with other operations that delimit scopes.

As noted in Section \ref{preliminaries:handler} non-algebraic and in particular
operations with scopes can be implemented as a handler.
As noted by \textcite{DBLP:conf/lics/PirogSWJ18}, this means that non-algebraic
operations always have associated semantics, while algebraic operations are just
syntax and are assigned meaning later by a handler.
Furthermore, \textcite{DBLP:conf/haskell/WuSH14} note a general problem when
working with multiple handlers describing scoped operations.
The order of handlers determines the semantics of a program, but simultaneously
the handlers are used to delimit scopes.
Therefore, their position are fixed.
Problems arise if the ordering for certain semantics and the forced positions
for delimiting scopes do not coincide.
It is either impossible to model certain semantics or one has to evaluate
handlers multiple times, reinjecting intermediate results into the
computation.
\textcite{DBLP:conf/haskell/WuSH14} note that the root of this problem is that
handlers combine syntax and semantics by delimiting scopes as well as
interpreting syntax.

% TODO: parser example + hand waving?

To fix this problem \textcite{DBLP:conf/haskell/WuSH14} introduced two solutions
for syntactically delimiting scopes.
Using this new syntax, the handlers are freed from delimiting scopes and are
again purely semantics constructs and can be moved freely to the outside of the
program.
By expanding the syntax, the handling of effects becomes more difficult.
In Chapter \ref{chapter:first-order} and Chapter \ref{chapter:higher-order} we
explain these two approaches and discuss how well they translate from Haskell to
Agda.
